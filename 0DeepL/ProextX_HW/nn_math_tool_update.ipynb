{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural network, experimentation tool, version 1</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# just copy/paste -the needed activation functions, \n",
    "# we're going to need these again\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness\n",
    "# np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        # add the row to result (y is target)\n",
    "        # basically the order of variables: x1, x2, y \n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The neural network training code</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss 33.284138739057106\n",
      "Epoch: 2, loss 26.281665173565415\n",
      "Epoch: 3, loss 25.969254693658947\n",
      "Epoch: 4, loss 25.95444531619219\n",
      "Epoch: 5, loss 25.953741198355623\n",
      "Epoch: 6, loss 25.95370771602454\n",
      "Epoch: 7, loss 25.9537061238562\n",
      "Epoch: 8, loss 25.95370604814464\n",
      "Epoch: 9, loss 25.953706044544326\n",
      "Epoch: 10, loss 25.953706044373106\n",
      "Epoch: 11, loss 25.953706044365017\n",
      "Epoch: 12, loss 25.953706044364655\n",
      "Epoch: 13, loss 25.95370604436462\n",
      "Epoch: 14, loss 25.9537060443646\n",
      "Epoch: 15, loss 25.9537060443646\n",
      "Epoch: 16, loss 25.9537060443646\n",
      "Epoch: 17, loss 25.9537060443646\n",
      "Epoch: 18, loss 25.9537060443646\n",
      "Epoch: 19, loss 25.9537060443646\n",
      "Epoch: 20, loss 25.9537060443646\n",
      "Epoch: 21, loss 25.9537060443646\n",
      "Epoch: 22, loss 25.9537060443646\n",
      "Epoch: 23, loss 25.9537060443646\n",
      "Epoch: 24, loss 25.9537060443646\n",
      "Epoch: 25, loss 25.9537060443646\n",
      "Epoch: 26, loss 25.9537060443646\n",
      "Epoch: 27, loss 25.9537060443646\n",
      "Epoch: 28, loss 25.9537060443646\n",
      "Epoch: 29, loss 25.9537060443646\n",
      "Epoch: 30, loss 25.9537060443646\n",
      "Epoch: 31, loss 25.9537060443646\n",
      "Epoch: 32, loss 25.9537060443646\n",
      "Epoch: 33, loss 25.9537060443646\n",
      "Epoch: 34, loss 25.9537060443646\n",
      "Epoch: 35, loss 25.9537060443646\n",
      "Epoch: 36, loss 25.9537060443646\n",
      "Epoch: 37, loss 25.9537060443646\n",
      "Epoch: 38, loss 25.9537060443646\n",
      "Epoch: 39, loss 25.9537060443646\n",
      "Epoch: 40, loss 25.9537060443646\n",
      "Epoch: 41, loss 25.9537060443646\n",
      "Epoch: 42, loss 25.9537060443646\n",
      "Epoch: 43, loss 25.9537060443646\n",
      "Epoch: 44, loss 25.9537060443646\n",
      "Epoch: 45, loss 25.9537060443646\n",
      "Epoch: 46, loss 25.9537060443646\n",
      "Epoch: 47, loss 25.9537060443646\n",
      "Epoch: 48, loss 25.9537060443646\n",
      "Epoch: 49, loss 25.9537060443646\n",
      "Epoch: 50, loss 25.9537060443646\n",
      "Epoch: 51, loss 25.9537060443646\n",
      "Epoch: 52, loss 25.9537060443646\n",
      "Epoch: 53, loss 25.9537060443646\n",
      "Epoch: 54, loss 25.9537060443646\n",
      "Epoch: 55, loss 25.9537060443646\n",
      "Epoch: 56, loss 25.9537060443646\n",
      "Epoch: 57, loss 25.9537060443646\n",
      "Epoch: 58, loss 25.9537060443646\n",
      "Epoch: 59, loss 25.9537060443646\n",
      "Epoch: 60, loss 25.9537060443646\n",
      "Epoch: 61, loss 25.9537060443646\n",
      "Epoch: 62, loss 25.9537060443646\n",
      "Epoch: 63, loss 25.9537060443646\n",
      "Epoch: 64, loss 25.9537060443646\n",
      "Epoch: 65, loss 25.9537060443646\n",
      "Epoch: 66, loss 25.9537060443646\n",
      "Epoch: 67, loss 25.9537060443646\n",
      "Epoch: 68, loss 25.9537060443646\n",
      "Epoch: 69, loss 25.9537060443646\n",
      "Epoch: 70, loss 25.9537060443646\n",
      "Epoch: 71, loss 25.9537060443646\n",
      "Epoch: 72, loss 25.9537060443646\n",
      "Epoch: 73, loss 25.9537060443646\n",
      "Epoch: 74, loss 25.9537060443646\n",
      "Epoch: 75, loss 25.9537060443646\n",
      "Epoch: 76, loss 25.9537060443646\n",
      "Epoch: 77, loss 25.9537060443646\n",
      "Epoch: 78, loss 25.9537060443646\n",
      "Epoch: 79, loss 25.9537060443646\n",
      "Epoch: 80, loss 25.9537060443646\n",
      "Epoch: 81, loss 25.9537060443646\n",
      "Epoch: 82, loss 25.9537060443646\n",
      "Epoch: 83, loss 25.9537060443646\n",
      "Epoch: 84, loss 25.9537060443646\n",
      "Epoch: 85, loss 25.9537060443646\n",
      "Epoch: 86, loss 25.9537060443646\n",
      "Epoch: 87, loss 25.9537060443646\n",
      "Epoch: 88, loss 25.9537060443646\n",
      "Epoch: 89, loss 25.9537060443646\n",
      "Epoch: 90, loss 25.9537060443646\n",
      "Epoch: 91, loss 25.9537060443646\n",
      "Epoch: 92, loss 25.9537060443646\n",
      "Epoch: 93, loss 25.9537060443646\n",
      "Epoch: 94, loss 25.9537060443646\n",
      "Epoch: 95, loss 25.9537060443646\n",
      "Epoch: 96, loss 25.9537060443646\n",
      "Epoch: 97, loss 25.9537060443646\n",
      "Epoch: 98, loss 25.9537060443646\n",
      "Epoch: 99, loss 25.9537060443646\n",
      "Epoch: 100, loss 25.9537060443646\n",
      "Epoch: 101, loss 25.9537060443646\n",
      "Epoch: 102, loss 25.9537060443646\n",
      "Epoch: 103, loss 25.9537060443646\n",
      "Epoch: 104, loss 25.9537060443646\n",
      "Epoch: 105, loss 25.9537060443646\n",
      "Epoch: 106, loss 25.9537060443646\n",
      "Epoch: 107, loss 25.9537060443646\n",
      "Epoch: 108, loss 25.9537060443646\n",
      "Epoch: 109, loss 25.9537060443646\n",
      "Epoch: 110, loss 25.9537060443646\n",
      "Epoch: 111, loss 25.9537060443646\n",
      "Epoch: 112, loss 25.9537060443646\n",
      "Epoch: 113, loss 25.9537060443646\n",
      "Epoch: 114, loss 25.9537060443646\n",
      "Epoch: 115, loss 25.9537060443646\n",
      "Epoch: 116, loss 25.9537060443646\n",
      "Epoch: 117, loss 25.9537060443646\n",
      "Epoch: 118, loss 25.9537060443646\n",
      "Epoch: 119, loss 25.9537060443646\n",
      "Epoch: 120, loss 25.9537060443646\n",
      "Epoch: 121, loss 25.9537060443646\n",
      "Epoch: 122, loss 25.9537060443646\n",
      "Epoch: 123, loss 25.9537060443646\n",
      "Epoch: 124, loss 25.9537060443646\n",
      "Epoch: 125, loss 25.9537060443646\n",
      "Epoch: 126, loss 25.9537060443646\n",
      "Epoch: 127, loss 25.9537060443646\n",
      "Epoch: 128, loss 25.9537060443646\n",
      "Epoch: 129, loss 25.9537060443646\n",
      "Epoch: 130, loss 25.9537060443646\n",
      "Epoch: 131, loss 25.9537060443646\n",
      "Epoch: 132, loss 25.9537060443646\n",
      "Epoch: 133, loss 25.9537060443646\n",
      "Epoch: 134, loss 25.9537060443646\n",
      "Epoch: 135, loss 25.9537060443646\n",
      "Epoch: 136, loss 25.9537060443646\n",
      "Epoch: 137, loss 25.9537060443646\n",
      "Epoch: 138, loss 25.9537060443646\n",
      "Epoch: 139, loss 25.9537060443646\n",
      "Epoch: 140, loss 25.9537060443646\n",
      "Epoch: 141, loss 25.9537060443646\n",
      "Epoch: 142, loss 25.9537060443646\n",
      "Epoch: 143, loss 25.9537060443646\n",
      "Epoch: 144, loss 25.9537060443646\n",
      "Epoch: 145, loss 25.9537060443646\n",
      "Epoch: 146, loss 25.9537060443646\n",
      "Epoch: 147, loss 25.9537060443646\n",
      "Epoch: 148, loss 25.9537060443646\n",
      "Epoch: 149, loss 25.9537060443646\n",
      "Epoch: 150, loss 25.9537060443646\n",
      "Epoch: 151, loss 25.9537060443646\n",
      "Epoch: 152, loss 25.9537060443646\n",
      "Epoch: 153, loss 25.9537060443646\n",
      "Epoch: 154, loss 25.9537060443646\n",
      "Epoch: 155, loss 25.9537060443646\n",
      "Epoch: 156, loss 25.9537060443646\n",
      "Epoch: 157, loss 25.9537060443646\n",
      "Epoch: 158, loss 25.9537060443646\n",
      "Epoch: 159, loss 25.9537060443646\n",
      "Epoch: 160, loss 25.9537060443646\n",
      "Epoch: 161, loss 25.9537060443646\n",
      "Epoch: 162, loss 25.9537060443646\n",
      "Epoch: 163, loss 25.9537060443646\n",
      "Epoch: 164, loss 25.9537060443646\n",
      "Epoch: 165, loss 25.9537060443646\n",
      "Epoch: 166, loss 25.9537060443646\n",
      "Epoch: 167, loss 25.9537060443646\n",
      "Epoch: 168, loss 25.9537060443646\n",
      "Epoch: 169, loss 25.9537060443646\n",
      "Epoch: 170, loss 25.9537060443646\n",
      "Epoch: 171, loss 25.9537060443646\n",
      "Epoch: 172, loss 25.9537060443646\n",
      "Epoch: 173, loss 25.9537060443646\n",
      "Epoch: 174, loss 25.9537060443646\n",
      "Epoch: 175, loss 25.9537060443646\n",
      "Epoch: 176, loss 25.9537060443646\n",
      "Epoch: 177, loss 25.9537060443646\n",
      "Epoch: 178, loss 25.9537060443646\n",
      "Epoch: 179, loss 25.9537060443646\n",
      "Epoch: 180, loss 25.9537060443646\n",
      "Epoch: 181, loss 25.9537060443646\n",
      "Epoch: 182, loss 25.9537060443646\n",
      "Epoch: 183, loss 25.9537060443646\n",
      "Epoch: 184, loss 25.9537060443646\n",
      "Epoch: 185, loss 25.9537060443646\n",
      "Epoch: 186, loss 25.9537060443646\n",
      "Epoch: 187, loss 25.9537060443646\n",
      "Epoch: 188, loss 25.9537060443646\n",
      "Epoch: 189, loss 25.9537060443646\n",
      "Epoch: 190, loss 25.9537060443646\n",
      "Epoch: 191, loss 25.9537060443646\n",
      "Epoch: 192, loss 25.9537060443646\n",
      "Epoch: 193, loss 25.9537060443646\n",
      "Epoch: 194, loss 25.9537060443646\n",
      "Epoch: 195, loss 25.9537060443646\n",
      "Epoch: 196, loss 25.9537060443646\n",
      "Epoch: 197, loss 25.9537060443646\n",
      "Epoch: 198, loss 25.9537060443646\n",
      "Epoch: 199, loss 25.9537060443646\n",
      "Epoch: 200, loss 25.9537060443646\n",
      "Epoch: 201, loss 25.9537060443646\n",
      "Epoch: 202, loss 25.9537060443646\n",
      "Epoch: 203, loss 25.9537060443646\n",
      "Epoch: 204, loss 25.9537060443646\n",
      "Epoch: 205, loss 25.9537060443646\n",
      "Epoch: 206, loss 25.9537060443646\n",
      "Epoch: 207, loss 25.9537060443646\n",
      "Epoch: 208, loss 25.9537060443646\n",
      "Epoch: 209, loss 25.9537060443646\n",
      "Epoch: 210, loss 25.9537060443646\n",
      "Epoch: 211, loss 25.9537060443646\n",
      "Epoch: 212, loss 25.9537060443646\n",
      "Epoch: 213, loss 25.9537060443646\n",
      "Epoch: 214, loss 25.9537060443646\n",
      "Epoch: 215, loss 25.9537060443646\n",
      "Epoch: 216, loss 25.9537060443646\n",
      "Epoch: 217, loss 25.9537060443646\n",
      "Epoch: 218, loss 25.9537060443646\n",
      "Epoch: 219, loss 25.9537060443646\n",
      "Epoch: 220, loss 25.9537060443646\n",
      "Epoch: 221, loss 25.9537060443646\n",
      "Epoch: 222, loss 25.9537060443646\n",
      "Epoch: 223, loss 25.9537060443646\n",
      "Epoch: 224, loss 25.9537060443646\n",
      "Epoch: 225, loss 25.9537060443646\n",
      "Epoch: 226, loss 25.9537060443646\n",
      "Epoch: 227, loss 25.9537060443646\n",
      "Epoch: 228, loss 25.9537060443646\n",
      "Epoch: 229, loss 25.9537060443646\n",
      "Epoch: 230, loss 25.9537060443646\n",
      "Epoch: 231, loss 25.9537060443646\n",
      "Epoch: 232, loss 25.9537060443646\n",
      "Epoch: 233, loss 25.9537060443646\n",
      "Epoch: 234, loss 25.9537060443646\n",
      "Epoch: 235, loss 25.9537060443646\n",
      "Epoch: 236, loss 25.9537060443646\n",
      "Epoch: 237, loss 25.9537060443646\n",
      "Epoch: 238, loss 25.9537060443646\n",
      "Epoch: 239, loss 25.9537060443646\n",
      "Epoch: 240, loss 25.9537060443646\n",
      "Epoch: 241, loss 25.9537060443646\n",
      "Epoch: 242, loss 25.9537060443646\n",
      "Epoch: 243, loss 25.9537060443646\n",
      "Epoch: 244, loss 25.9537060443646\n",
      "Epoch: 245, loss 25.9537060443646\n",
      "Epoch: 246, loss 25.9537060443646\n",
      "Epoch: 247, loss 25.9537060443646\n",
      "Epoch: 248, loss 25.9537060443646\n",
      "Epoch: 249, loss 25.9537060443646\n",
      "Epoch: 250, loss 25.9537060443646\n",
      "Epoch: 251, loss 25.9537060443646\n",
      "Epoch: 252, loss 25.9537060443646\n",
      "Epoch: 253, loss 25.9537060443646\n",
      "Epoch: 254, loss 25.9537060443646\n",
      "Epoch: 255, loss 25.9537060443646\n",
      "Epoch: 256, loss 25.9537060443646\n",
      "Epoch: 257, loss 25.9537060443646\n",
      "Epoch: 258, loss 25.9537060443646\n",
      "Epoch: 259, loss 25.9537060443646\n",
      "Epoch: 260, loss 25.9537060443646\n",
      "Epoch: 261, loss 25.9537060443646\n",
      "Epoch: 262, loss 25.9537060443646\n",
      "Epoch: 263, loss 25.9537060443646\n",
      "Epoch: 264, loss 25.9537060443646\n",
      "Epoch: 265, loss 25.9537060443646\n",
      "Epoch: 266, loss 25.9537060443646\n",
      "Epoch: 267, loss 25.9537060443646\n",
      "Epoch: 268, loss 25.9537060443646\n",
      "Epoch: 269, loss 25.9537060443646\n",
      "Epoch: 270, loss 25.9537060443646\n",
      "Epoch: 271, loss 25.9537060443646\n",
      "Epoch: 272, loss 25.9537060443646\n",
      "Epoch: 273, loss 25.9537060443646\n",
      "Epoch: 274, loss 25.9537060443646\n",
      "Epoch: 275, loss 25.9537060443646\n",
      "Epoch: 276, loss 25.9537060443646\n",
      "Epoch: 277, loss 25.9537060443646\n",
      "Epoch: 278, loss 25.9537060443646\n",
      "Epoch: 279, loss 25.9537060443646\n",
      "Epoch: 280, loss 25.9537060443646\n",
      "Epoch: 281, loss 25.9537060443646\n",
      "Epoch: 282, loss 25.9537060443646\n",
      "Epoch: 283, loss 25.9537060443646\n",
      "Epoch: 284, loss 25.9537060443646\n",
      "Epoch: 285, loss 25.9537060443646\n",
      "Epoch: 286, loss 25.9537060443646\n",
      "Epoch: 287, loss 25.9537060443646\n",
      "Epoch: 288, loss 25.9537060443646\n",
      "Epoch: 289, loss 25.9537060443646\n",
      "Epoch: 290, loss 25.9537060443646\n",
      "Epoch: 291, loss 25.9537060443646\n",
      "Epoch: 292, loss 25.9537060443646\n",
      "Epoch: 293, loss 25.9537060443646\n",
      "Epoch: 294, loss 25.9537060443646\n",
      "Epoch: 295, loss 25.9537060443646\n",
      "Epoch: 296, loss 25.9537060443646\n",
      "Epoch: 297, loss 25.9537060443646\n",
      "Epoch: 298, loss 25.9537060443646\n",
      "Epoch: 299, loss 25.9537060443646\n",
      "Epoch: 300, loss 25.9537060443646\n",
      "Epoch: 301, loss 25.9537060443646\n",
      "Epoch: 302, loss 25.9537060443646\n",
      "Epoch: 303, loss 25.9537060443646\n",
      "Epoch: 304, loss 25.9537060443646\n",
      "Epoch: 305, loss 25.9537060443646\n",
      "Epoch: 306, loss 25.9537060443646\n",
      "Epoch: 307, loss 25.9537060443646\n",
      "Epoch: 308, loss 25.9537060443646\n",
      "Epoch: 309, loss 25.9537060443646\n",
      "Epoch: 310, loss 25.9537060443646\n",
      "Epoch: 311, loss 25.9537060443646\n",
      "Epoch: 312, loss 25.9537060443646\n",
      "Epoch: 313, loss 25.9537060443646\n",
      "Epoch: 314, loss 25.9537060443646\n",
      "Epoch: 315, loss 25.9537060443646\n",
      "Epoch: 316, loss 25.9537060443646\n",
      "Epoch: 317, loss 25.9537060443646\n",
      "Epoch: 318, loss 25.9537060443646\n",
      "Epoch: 319, loss 25.9537060443646\n",
      "Epoch: 320, loss 25.9537060443646\n",
      "Epoch: 321, loss 25.9537060443646\n",
      "Epoch: 322, loss 25.9537060443646\n",
      "Epoch: 323, loss 25.9537060443646\n",
      "Epoch: 324, loss 25.9537060443646\n",
      "Epoch: 325, loss 25.9537060443646\n",
      "Epoch: 326, loss 25.9537060443646\n",
      "Epoch: 327, loss 25.9537060443646\n",
      "Epoch: 328, loss 25.9537060443646\n",
      "Epoch: 329, loss 25.9537060443646\n",
      "Epoch: 330, loss 25.9537060443646\n",
      "Epoch: 331, loss 25.9537060443646\n",
      "Epoch: 332, loss 25.9537060443646\n",
      "Epoch: 333, loss 25.9537060443646\n",
      "Epoch: 334, loss 25.9537060443646\n",
      "Epoch: 335, loss 25.9537060443646\n",
      "Epoch: 336, loss 25.9537060443646\n",
      "Epoch: 337, loss 25.9537060443646\n",
      "Epoch: 338, loss 25.9537060443646\n",
      "Epoch: 339, loss 25.9537060443646\n",
      "Epoch: 340, loss 25.9537060443646\n",
      "Epoch: 341, loss 25.9537060443646\n",
      "Epoch: 342, loss 25.9537060443646\n",
      "Epoch: 343, loss 25.9537060443646\n",
      "Epoch: 344, loss 25.9537060443646\n",
      "Epoch: 345, loss 25.9537060443646\n",
      "Epoch: 346, loss 25.9537060443646\n",
      "Epoch: 347, loss 25.9537060443646\n",
      "Epoch: 348, loss 25.9537060443646\n",
      "Epoch: 349, loss 25.9537060443646\n",
      "Epoch: 350, loss 25.9537060443646\n",
      "Epoch: 351, loss 25.9537060443646\n",
      "Epoch: 352, loss 25.9537060443646\n",
      "Epoch: 353, loss 25.9537060443646\n",
      "Epoch: 354, loss 25.9537060443646\n",
      "Epoch: 355, loss 25.9537060443646\n",
      "Epoch: 356, loss 25.9537060443646\n",
      "Epoch: 357, loss 25.9537060443646\n",
      "Epoch: 358, loss 25.9537060443646\n",
      "Epoch: 359, loss 25.9537060443646\n",
      "Epoch: 360, loss 25.9537060443646\n",
      "Epoch: 361, loss 25.9537060443646\n",
      "Epoch: 362, loss 25.9537060443646\n",
      "Epoch: 363, loss 25.9537060443646\n",
      "Epoch: 364, loss 25.9537060443646\n",
      "Epoch: 365, loss 25.9537060443646\n",
      "Epoch: 366, loss 25.9537060443646\n",
      "Epoch: 367, loss 25.9537060443646\n",
      "Epoch: 368, loss 25.9537060443646\n",
      "Epoch: 369, loss 25.9537060443646\n",
      "Epoch: 370, loss 25.9537060443646\n",
      "Epoch: 371, loss 25.9537060443646\n",
      "Epoch: 372, loss 25.9537060443646\n",
      "Epoch: 373, loss 25.9537060443646\n",
      "Epoch: 374, loss 25.9537060443646\n",
      "Epoch: 375, loss 25.9537060443646\n",
      "Epoch: 376, loss 25.9537060443646\n",
      "Epoch: 377, loss 25.9537060443646\n",
      "Epoch: 378, loss 25.9537060443646\n",
      "Epoch: 379, loss 25.9537060443646\n",
      "Epoch: 380, loss 25.9537060443646\n",
      "Epoch: 381, loss 25.9537060443646\n",
      "Epoch: 382, loss 25.9537060443646\n",
      "Epoch: 383, loss 25.9537060443646\n",
      "Epoch: 384, loss 25.9537060443646\n",
      "Epoch: 385, loss 25.9537060443646\n",
      "Epoch: 386, loss 25.9537060443646\n",
      "Epoch: 387, loss 25.9537060443646\n",
      "Epoch: 388, loss 25.9537060443646\n",
      "Epoch: 389, loss 25.9537060443646\n",
      "Epoch: 390, loss 25.9537060443646\n",
      "Epoch: 391, loss 25.9537060443646\n",
      "Epoch: 392, loss 25.9537060443646\n",
      "Epoch: 393, loss 25.9537060443646\n",
      "Epoch: 394, loss 25.9537060443646\n",
      "Epoch: 395, loss 25.9537060443646\n",
      "Epoch: 396, loss 25.9537060443646\n",
      "Epoch: 397, loss 25.9537060443646\n",
      "Epoch: 398, loss 25.9537060443646\n",
      "Epoch: 399, loss 25.9537060443646\n",
      "Epoch: 400, loss 25.9537060443646\n"
     ]
    }
   ],
   "source": [
    "# we'll start building our neural network training app here\n",
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1.5\n",
    "w2 = 0.5\n",
    "w3 = -2\n",
    "w4 = -0.5\n",
    "w5 = 1.5\n",
    "w6 = 1.2\n",
    "bias1 = 0.5\n",
    "bias2 = -0.35\n",
    "bias3 = 0.5\n",
    "\n",
    "# we'll save these for future\n",
    "# se we can compare results to the final weights\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    " \n",
    "# learning rate and epochs\n",
    "LR = 0.015\n",
    "epochs = 400\n",
    "\n",
    "\n",
    "# override above data with our generation function\n",
    "data = generate_train_data()\n",
    "\n",
    "\n",
    "\n",
    "# let's initialize a list for loss points\n",
    "loss_points = []\n",
    "\n",
    "# START THE TRAINING PROCESS\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # let's also monitor epoch-wise losses\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # for example with first row\n",
    "        # [1, 0, 2] => assign input1 = 1, input2 = 0, true_value = 2\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # FORWARD PASS\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the the previous weights\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        # probably used later, we might want to have error metrics (MSE)\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current training data row loss to epoch losses\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solve w5 and update the new value\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        # solve w6 and update the new value\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        # solve bias3 and update the new value\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT FORWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES -> CHAIN RULE\n",
    "\n",
    "        # see the materials and the math experiment notebook for more details\n",
    "        # start with weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS!\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it the loss_points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the average loss of this epoch into the overall loss list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.5\n",
      "w2: 0.5\n",
      "w3: -2\n",
      "w4: -0.5\n",
      "w5: 1.5\n",
      "w6: 1.2\n",
      "b1: 0.5\n",
      "b2: -0.35\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: -2.6431141092382093\n",
      "w2: -2.1676542092528868\n",
      "w3: -14.794699468476418\n",
      "w4: -9.442085418505773\n",
      "w5: -28.260329945482837\n",
      "w6: -28.13872712552198\n",
      "b1: -1.1816027864127365\n",
      "b2: -1.4796698697509623\n",
      "b3: 12.058356344581021\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # FORWARD PASS\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the the previous weights\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 9]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 5, 6]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.058356344581021"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # the value tends to be same as final bias3 \n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.03846154, 0.23076923)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.058356344581021"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the value tends to be same as final bias3 \n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "predict(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.058356344581021"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(30,150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
