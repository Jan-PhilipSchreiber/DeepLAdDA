{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural network, Data generation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# just copy/paste -the needed activation functions, \n",
    "# we're going to need these again\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_data(num_rows=2000, seed=123):\n",
    "    \"\"\"\n",
    "    Generates training data with a mix of uniform and normal distributions for realism.\n",
    "    Args:\n",
    "        num_rows (int): Number of rows to generate.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "    Returns:\n",
    "        list: A list of [x1, x2, y] rows.\n",
    "    \"\"\"\n",
    "    # Lock randomness for reproducibility\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for _ in range(num_rows):\n",
    "        # Generate features using a mix of distributions\n",
    "        x1 = np.random.normal(loc=5, scale=2)  # Normally distributed around 5 with std 2\n",
    "        x2 = np.random.uniform(5, 15)          # Uniform distribution between 5 and 15\n",
    "        \n",
    "        # Clip x1 to keep values within a reasonable range\n",
    "        x1 = max(0, min(10, x1))  # Ensure x1 stays between 0 and 10\n",
    "        \n",
    "        # Add additional variability in the target variable (y)\n",
    "        noise = np.random.normal(loc=0, scale=3)  # Gaussian noise with mean 0 and std 3\n",
    "        y = x1 ** 2 + x2 + noise\n",
    "        \n",
    "        # Append the row to the result\n",
    "        result.append([x1, x2, y])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The neural network training code</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss 1469.7744571915914\n",
      "Epoch: 2, loss 1469.7744621319648\n",
      "Epoch: 3, loss 1469.7744621319648\n",
      "Epoch: 4, loss 1469.7744621319648\n",
      "Epoch: 5, loss 1469.7744621319648\n",
      "Epoch: 6, loss 1469.7744621319648\n",
      "Epoch: 7, loss 1469.7744621319648\n",
      "Epoch: 8, loss 1469.7744621319648\n",
      "Epoch: 9, loss 1469.7744621319648\n",
      "Epoch: 10, loss 1469.7744621319648\n",
      "Epoch: 11, loss 1469.7744621319648\n",
      "Epoch: 12, loss 1469.7744621319648\n",
      "Epoch: 13, loss 1469.7744621319648\n",
      "Epoch: 14, loss 1469.7744621319648\n",
      "Epoch: 15, loss 1469.7744621319648\n",
      "Epoch: 16, loss 1469.7744621319648\n",
      "Epoch: 17, loss 1469.7744621319648\n",
      "Epoch: 18, loss 1469.7744621319648\n",
      "Epoch: 19, loss 1469.7744621319648\n",
      "Epoch: 20, loss 1469.7744621319648\n",
      "Epoch: 21, loss 1469.7744621319648\n",
      "Epoch: 22, loss 1469.7744621319648\n",
      "Epoch: 23, loss 1469.7744621319648\n",
      "Epoch: 24, loss 1469.7744621319648\n",
      "Epoch: 25, loss 1469.7744621319648\n",
      "Epoch: 26, loss 1469.7744621319648\n",
      "Epoch: 27, loss 1469.7744621319648\n",
      "Epoch: 28, loss 1469.7744621319648\n",
      "Epoch: 29, loss 1469.7744621319648\n",
      "Epoch: 30, loss 1469.7744621319648\n",
      "Epoch: 31, loss 1469.7744621319648\n",
      "Epoch: 32, loss 1469.7744621319648\n",
      "Epoch: 33, loss 1469.7744621319648\n",
      "Epoch: 34, loss 1469.7744621319648\n",
      "Epoch: 35, loss 1469.7744621319648\n",
      "Epoch: 36, loss 1469.7744621319648\n",
      "Epoch: 37, loss 1469.7744621319648\n",
      "Epoch: 38, loss 1469.7744621319648\n",
      "Epoch: 39, loss 1469.7744621319648\n",
      "Epoch: 40, loss 1469.7744621319648\n",
      "Epoch: 41, loss 1469.7744621319648\n",
      "Epoch: 42, loss 1469.7744621319648\n",
      "Epoch: 43, loss 1469.7744621319648\n",
      "Epoch: 44, loss 1469.7744621319648\n",
      "Epoch: 45, loss 1469.7744621319648\n",
      "Epoch: 46, loss 1469.7744621319648\n",
      "Epoch: 47, loss 1469.7744621319648\n",
      "Epoch: 48, loss 1469.7744621319648\n",
      "Epoch: 49, loss 1469.7744621319648\n",
      "Epoch: 50, loss 1469.7744621319648\n",
      "Epoch: 51, loss 1469.7744621319648\n",
      "Epoch: 52, loss 1469.7744621319648\n",
      "Epoch: 53, loss 1469.7744621319648\n",
      "Epoch: 54, loss 1469.7744621319648\n",
      "Epoch: 55, loss 1469.7744621319648\n",
      "Epoch: 56, loss 1469.7744621319648\n",
      "Epoch: 57, loss 1469.7744621319648\n",
      "Epoch: 58, loss 1469.7744621319648\n",
      "Epoch: 59, loss 1469.7744621319648\n",
      "Epoch: 60, loss 1469.7744621319648\n",
      "Epoch: 61, loss 1469.7744621319648\n",
      "Epoch: 62, loss 1469.7744621319648\n",
      "Epoch: 63, loss 1469.7744621319648\n",
      "Epoch: 64, loss 1469.7744621319648\n",
      "Epoch: 65, loss 1469.7744621319648\n",
      "Epoch: 66, loss 1469.7744621319648\n",
      "Epoch: 67, loss 1469.7744621319648\n",
      "Epoch: 68, loss 1469.7744621319648\n",
      "Epoch: 69, loss 1469.7744621319648\n",
      "Epoch: 70, loss 1469.7744621319648\n",
      "Epoch: 71, loss 1469.7744621319648\n",
      "Epoch: 72, loss 1469.7744621319648\n",
      "Epoch: 73, loss 1469.7744621319648\n",
      "Epoch: 74, loss 1469.7744621319648\n",
      "Epoch: 75, loss 1469.7744621319648\n",
      "Epoch: 76, loss 1469.7744621319648\n",
      "Epoch: 77, loss 1469.7744621319648\n",
      "Epoch: 78, loss 1469.7744621319648\n",
      "Epoch: 79, loss 1469.7744621319648\n",
      "Epoch: 80, loss 1469.7744621319648\n",
      "Epoch: 81, loss 1469.7744621319648\n",
      "Epoch: 82, loss 1469.7744621319648\n",
      "Epoch: 83, loss 1469.7744621319648\n",
      "Epoch: 84, loss 1469.7744621319648\n",
      "Epoch: 85, loss 1469.7744621319648\n",
      "Epoch: 86, loss 1469.7744621319648\n",
      "Epoch: 87, loss 1469.7744621319648\n",
      "Epoch: 88, loss 1469.7744621319648\n",
      "Epoch: 89, loss 1469.7744621319648\n",
      "Epoch: 90, loss 1469.7744621319648\n",
      "Epoch: 91, loss 1469.7744621319648\n",
      "Epoch: 92, loss 1469.7744621319648\n",
      "Epoch: 93, loss 1469.7744621319648\n",
      "Epoch: 94, loss 1469.7744621319648\n",
      "Epoch: 95, loss 1469.7744621319648\n",
      "Epoch: 96, loss 1469.7744621319648\n",
      "Epoch: 97, loss 1469.7744621319648\n",
      "Epoch: 98, loss 1469.7744621319648\n",
      "Epoch: 99, loss 1469.7744621319648\n",
      "Epoch: 100, loss 1469.7744621319648\n",
      "Epoch: 101, loss 1469.7744621319648\n",
      "Epoch: 102, loss 1469.7744621319648\n",
      "Epoch: 103, loss 1469.7744621319648\n",
      "Epoch: 104, loss 1469.7744621319648\n",
      "Epoch: 105, loss 1469.7744621319648\n",
      "Epoch: 106, loss 1469.7744621319648\n",
      "Epoch: 107, loss 1469.7744621319648\n",
      "Epoch: 108, loss 1469.7744621319648\n",
      "Epoch: 109, loss 1469.7744621319648\n",
      "Epoch: 110, loss 1469.7744621319648\n",
      "Epoch: 111, loss 1469.7744621319648\n",
      "Epoch: 112, loss 1469.7744621319648\n",
      "Epoch: 113, loss 1469.7744621319648\n",
      "Epoch: 114, loss 1469.7744621319648\n",
      "Epoch: 115, loss 1469.7744621319648\n",
      "Epoch: 116, loss 1469.7744621319648\n",
      "Epoch: 117, loss 1469.7744621319648\n",
      "Epoch: 118, loss 1469.7744621319648\n",
      "Epoch: 119, loss 1469.7744621319648\n",
      "Epoch: 120, loss 1469.7744621319648\n",
      "Epoch: 121, loss 1469.7744621319648\n",
      "Epoch: 122, loss 1469.7744621319648\n",
      "Epoch: 123, loss 1469.7744621319648\n",
      "Epoch: 124, loss 1469.7744621319648\n",
      "Epoch: 125, loss 1469.7744621319648\n",
      "Epoch: 126, loss 1469.7744621319648\n",
      "Epoch: 127, loss 1469.7744621319648\n",
      "Epoch: 128, loss 1469.7744621319648\n",
      "Epoch: 129, loss 1469.7744621319648\n",
      "Epoch: 130, loss 1469.7744621319648\n",
      "Epoch: 131, loss 1469.7744621319648\n",
      "Epoch: 132, loss 1469.7744621319648\n",
      "Epoch: 133, loss 1469.7744621319648\n",
      "Epoch: 134, loss 1469.7744621319648\n",
      "Epoch: 135, loss 1469.7744621319648\n",
      "Epoch: 136, loss 1469.7744621319648\n",
      "Epoch: 137, loss 1469.7744621319648\n",
      "Epoch: 138, loss 1469.7744621319648\n",
      "Epoch: 139, loss 1469.7744621319648\n",
      "Epoch: 140, loss 1469.7744621319648\n",
      "Epoch: 141, loss 1469.7744621319648\n",
      "Epoch: 142, loss 1469.7744621319648\n",
      "Epoch: 143, loss 1469.7744621319648\n",
      "Epoch: 144, loss 1469.7744621319648\n",
      "Epoch: 145, loss 1469.7744621319648\n",
      "Epoch: 146, loss 1469.7744621319648\n",
      "Epoch: 147, loss 1469.7744621319648\n",
      "Epoch: 148, loss 1469.7744621319648\n",
      "Epoch: 149, loss 1469.7744621319648\n",
      "Epoch: 150, loss 1469.7744621319648\n",
      "Epoch: 151, loss 1469.7744621319648\n",
      "Epoch: 152, loss 1469.7744621319648\n",
      "Epoch: 153, loss 1469.7744621319648\n",
      "Epoch: 154, loss 1469.7744621319648\n",
      "Epoch: 155, loss 1469.7744621319648\n",
      "Epoch: 156, loss 1469.7744621319648\n",
      "Epoch: 157, loss 1469.7744621319648\n",
      "Epoch: 158, loss 1469.7744621319648\n",
      "Epoch: 159, loss 1469.7744621319648\n",
      "Epoch: 160, loss 1469.7744621319648\n",
      "Epoch: 161, loss 1469.7744621319648\n",
      "Epoch: 162, loss 1469.7744621319648\n",
      "Epoch: 163, loss 1469.7744621319648\n",
      "Epoch: 164, loss 1469.7744621319648\n",
      "Epoch: 165, loss 1469.7744621319648\n",
      "Epoch: 166, loss 1469.7744621319648\n",
      "Epoch: 167, loss 1469.7744621319648\n",
      "Epoch: 168, loss 1469.7744621319648\n",
      "Epoch: 169, loss 1469.7744621319648\n",
      "Epoch: 170, loss 1469.7744621319648\n",
      "Epoch: 171, loss 1469.7744621319648\n",
      "Epoch: 172, loss 1469.7744621319648\n",
      "Epoch: 173, loss 1469.7744621319648\n",
      "Epoch: 174, loss 1469.7744621319648\n",
      "Epoch: 175, loss 1469.7744621319648\n",
      "Epoch: 176, loss 1469.7744621319648\n",
      "Epoch: 177, loss 1469.7744621319648\n",
      "Epoch: 178, loss 1469.7744621319648\n",
      "Epoch: 179, loss 1469.7744621319648\n",
      "Epoch: 180, loss 1469.7744621319648\n",
      "Epoch: 181, loss 1469.7744621319648\n",
      "Epoch: 182, loss 1469.7744621319648\n",
      "Epoch: 183, loss 1469.7744621319648\n",
      "Epoch: 184, loss 1469.7744621319648\n",
      "Epoch: 185, loss 1469.7744621319648\n",
      "Epoch: 186, loss 1469.7744621319648\n",
      "Epoch: 187, loss 1469.7744621319648\n",
      "Epoch: 188, loss 1469.7744621319648\n",
      "Epoch: 189, loss 1469.7744621319648\n",
      "Epoch: 190, loss 1469.7744621319648\n",
      "Epoch: 191, loss 1469.7744621319648\n",
      "Epoch: 192, loss 1469.7744621319648\n",
      "Epoch: 193, loss 1469.7744621319648\n",
      "Epoch: 194, loss 1469.7744621319648\n",
      "Epoch: 195, loss 1469.7744621319648\n",
      "Epoch: 196, loss 1469.7744621319648\n",
      "Epoch: 197, loss 1469.7744621319648\n",
      "Epoch: 198, loss 1469.7744621319648\n",
      "Epoch: 199, loss 1469.7744621319648\n",
      "Epoch: 200, loss 1469.7744621319648\n",
      "Epoch: 201, loss 1469.7744621319648\n",
      "Epoch: 202, loss 1469.7744621319648\n",
      "Epoch: 203, loss 1469.7744621319648\n",
      "Epoch: 204, loss 1469.7744621319648\n",
      "Epoch: 205, loss 1469.7744621319648\n",
      "Epoch: 206, loss 1469.7744621319648\n",
      "Epoch: 207, loss 1469.7744621319648\n",
      "Epoch: 208, loss 1469.7744621319648\n",
      "Epoch: 209, loss 1469.7744621319648\n",
      "Epoch: 210, loss 1469.7744621319648\n",
      "Epoch: 211, loss 1469.7744621319648\n",
      "Epoch: 212, loss 1469.7744621319648\n",
      "Epoch: 213, loss 1469.7744621319648\n",
      "Epoch: 214, loss 1469.7744621319648\n",
      "Epoch: 215, loss 1469.7744621319648\n",
      "Epoch: 216, loss 1469.7744621319648\n",
      "Epoch: 217, loss 1469.7744621319648\n",
      "Epoch: 218, loss 1469.7744621319648\n",
      "Epoch: 219, loss 1469.7744621319648\n",
      "Epoch: 220, loss 1469.7744621319648\n",
      "Epoch: 221, loss 1469.7744621319648\n",
      "Epoch: 222, loss 1469.7744621319648\n",
      "Epoch: 223, loss 1469.7744621319648\n",
      "Epoch: 224, loss 1469.7744621319648\n",
      "Epoch: 225, loss 1469.7744621319648\n",
      "Epoch: 226, loss 1469.7744621319648\n",
      "Epoch: 227, loss 1469.7744621319648\n",
      "Epoch: 228, loss 1469.7744621319648\n",
      "Epoch: 229, loss 1469.7744621319648\n",
      "Epoch: 230, loss 1469.7744621319648\n",
      "Epoch: 231, loss 1469.7744621319648\n",
      "Epoch: 232, loss 1469.7744621319648\n",
      "Epoch: 233, loss 1469.7744621319648\n",
      "Epoch: 234, loss 1469.7744621319648\n",
      "Epoch: 235, loss 1469.7744621319648\n",
      "Epoch: 236, loss 1469.7744621319648\n",
      "Epoch: 237, loss 1469.7744621319648\n",
      "Epoch: 238, loss 1469.7744621319648\n",
      "Epoch: 239, loss 1469.7744621319648\n",
      "Epoch: 240, loss 1469.7744621319648\n",
      "Epoch: 241, loss 1469.7744621319648\n",
      "Epoch: 242, loss 1469.7744621319648\n",
      "Epoch: 243, loss 1469.7744621319648\n",
      "Epoch: 244, loss 1469.7744621319648\n",
      "Epoch: 245, loss 1469.7744621319648\n",
      "Epoch: 246, loss 1469.7744621319648\n",
      "Epoch: 247, loss 1469.7744621319648\n",
      "Epoch: 248, loss 1469.7744621319648\n",
      "Epoch: 249, loss 1469.7744621319648\n",
      "Epoch: 250, loss 1469.7744621319648\n",
      "Epoch: 251, loss 1469.7744621319648\n",
      "Epoch: 252, loss 1469.7744621319648\n",
      "Epoch: 253, loss 1469.7744621319648\n",
      "Epoch: 254, loss 1469.7744621319648\n",
      "Epoch: 255, loss 1469.7744621319648\n",
      "Epoch: 256, loss 1469.7744621319648\n",
      "Epoch: 257, loss 1469.7744621319648\n",
      "Epoch: 258, loss 1469.7744621319648\n",
      "Epoch: 259, loss 1469.7744621319648\n",
      "Epoch: 260, loss 1469.7744621319648\n",
      "Epoch: 261, loss 1469.7744621319648\n",
      "Epoch: 262, loss 1469.7744621319648\n",
      "Epoch: 263, loss 1469.7744621319648\n",
      "Epoch: 264, loss 1469.7744621319648\n",
      "Epoch: 265, loss 1469.7744621319648\n",
      "Epoch: 266, loss 1469.7744621319648\n",
      "Epoch: 267, loss 1469.7744621319648\n",
      "Epoch: 268, loss 1469.7744621319648\n",
      "Epoch: 269, loss 1469.7744621319648\n",
      "Epoch: 270, loss 1469.7744621319648\n",
      "Epoch: 271, loss 1469.7744621319648\n",
      "Epoch: 272, loss 1469.7744621319648\n",
      "Epoch: 273, loss 1469.7744621319648\n",
      "Epoch: 274, loss 1469.7744621319648\n",
      "Epoch: 275, loss 1469.7744621319648\n",
      "Epoch: 276, loss 1469.7744621319648\n",
      "Epoch: 277, loss 1469.7744621319648\n",
      "Epoch: 278, loss 1469.7744621319648\n",
      "Epoch: 279, loss 1469.7744621319648\n",
      "Epoch: 280, loss 1469.7744621319648\n",
      "Epoch: 281, loss 1469.7744621319648\n",
      "Epoch: 282, loss 1469.7744621319648\n",
      "Epoch: 283, loss 1469.7744621319648\n",
      "Epoch: 284, loss 1469.7744621319648\n",
      "Epoch: 285, loss 1469.7744621319648\n",
      "Epoch: 286, loss 1469.7744621319648\n",
      "Epoch: 287, loss 1469.7744621319648\n",
      "Epoch: 288, loss 1469.7744621319648\n",
      "Epoch: 289, loss 1469.7744621319648\n",
      "Epoch: 290, loss 1469.7744621319648\n",
      "Epoch: 291, loss 1469.7744621319648\n",
      "Epoch: 292, loss 1469.7744621319648\n",
      "Epoch: 293, loss 1469.7744621319648\n",
      "Epoch: 294, loss 1469.7744621319648\n",
      "Epoch: 295, loss 1469.7744621319648\n",
      "Epoch: 296, loss 1469.7744621319648\n",
      "Epoch: 297, loss 1469.7744621319648\n",
      "Epoch: 298, loss 1469.7744621319648\n",
      "Epoch: 299, loss 1469.7744621319648\n",
      "Epoch: 300, loss 1469.7744621319648\n",
      "Epoch: 301, loss 1469.7744621319648\n",
      "Epoch: 302, loss 1469.7744621319648\n",
      "Epoch: 303, loss 1469.7744621319648\n",
      "Epoch: 304, loss 1469.7744621319648\n",
      "Epoch: 305, loss 1469.7744621319648\n",
      "Epoch: 306, loss 1469.7744621319648\n",
      "Epoch: 307, loss 1469.7744621319648\n",
      "Epoch: 308, loss 1469.7744621319648\n",
      "Epoch: 309, loss 1469.7744621319648\n",
      "Epoch: 310, loss 1469.7744621319648\n",
      "Epoch: 311, loss 1469.7744621319648\n",
      "Epoch: 312, loss 1469.7744621319648\n",
      "Epoch: 313, loss 1469.7744621319648\n",
      "Epoch: 314, loss 1469.7744621319648\n",
      "Epoch: 315, loss 1469.7744621319648\n",
      "Epoch: 316, loss 1469.7744621319648\n",
      "Epoch: 317, loss 1469.7744621319648\n",
      "Epoch: 318, loss 1469.7744621319648\n",
      "Epoch: 319, loss 1469.7744621319648\n",
      "Epoch: 320, loss 1469.7744621319648\n",
      "Epoch: 321, loss 1469.7744621319648\n",
      "Epoch: 322, loss 1469.7744621319648\n",
      "Epoch: 323, loss 1469.7744621319648\n",
      "Epoch: 324, loss 1469.7744621319648\n",
      "Epoch: 325, loss 1469.7744621319648\n",
      "Epoch: 326, loss 1469.7744621319648\n",
      "Epoch: 327, loss 1469.7744621319648\n",
      "Epoch: 328, loss 1469.7744621319648\n",
      "Epoch: 329, loss 1469.7744621319648\n",
      "Epoch: 330, loss 1469.7744621319648\n",
      "Epoch: 331, loss 1469.7744621319648\n",
      "Epoch: 332, loss 1469.7744621319648\n",
      "Epoch: 333, loss 1469.7744621319648\n",
      "Epoch: 334, loss 1469.7744621319648\n",
      "Epoch: 335, loss 1469.7744621319648\n",
      "Epoch: 336, loss 1469.7744621319648\n",
      "Epoch: 337, loss 1469.7744621319648\n",
      "Epoch: 338, loss 1469.7744621319648\n",
      "Epoch: 339, loss 1469.7744621319648\n",
      "Epoch: 340, loss 1469.7744621319648\n",
      "Epoch: 341, loss 1469.7744621319648\n",
      "Epoch: 342, loss 1469.7744621319648\n",
      "Epoch: 343, loss 1469.7744621319648\n",
      "Epoch: 344, loss 1469.7744621319648\n",
      "Epoch: 345, loss 1469.7744621319648\n",
      "Epoch: 346, loss 1469.7744621319648\n",
      "Epoch: 347, loss 1469.7744621319648\n",
      "Epoch: 348, loss 1469.7744621319648\n",
      "Epoch: 349, loss 1469.7744621319648\n",
      "Epoch: 350, loss 1469.7744621319648\n",
      "Epoch: 351, loss 1469.7744621319648\n",
      "Epoch: 352, loss 1469.7744621319648\n",
      "Epoch: 353, loss 1469.7744621319648\n",
      "Epoch: 354, loss 1469.7744621319648\n",
      "Epoch: 355, loss 1469.7744621319648\n",
      "Epoch: 356, loss 1469.7744621319648\n",
      "Epoch: 357, loss 1469.7744621319648\n",
      "Epoch: 358, loss 1469.7744621319648\n",
      "Epoch: 359, loss 1469.7744621319648\n",
      "Epoch: 360, loss 1469.7744621319648\n",
      "Epoch: 361, loss 1469.7744621319648\n",
      "Epoch: 362, loss 1469.7744621319648\n",
      "Epoch: 363, loss 1469.7744621319648\n",
      "Epoch: 364, loss 1469.7744621319648\n",
      "Epoch: 365, loss 1469.7744621319648\n",
      "Epoch: 366, loss 1469.7744621319648\n",
      "Epoch: 367, loss 1469.7744621319648\n",
      "Epoch: 368, loss 1469.7744621319648\n",
      "Epoch: 369, loss 1469.7744621319648\n",
      "Epoch: 370, loss 1469.7744621319648\n",
      "Epoch: 371, loss 1469.7744621319648\n",
      "Epoch: 372, loss 1469.7744621319648\n",
      "Epoch: 373, loss 1469.7744621319648\n",
      "Epoch: 374, loss 1469.7744621319648\n",
      "Epoch: 375, loss 1469.7744621319648\n",
      "Epoch: 376, loss 1469.7744621319648\n",
      "Epoch: 377, loss 1469.7744621319648\n",
      "Epoch: 378, loss 1469.7744621319648\n",
      "Epoch: 379, loss 1469.7744621319648\n",
      "Epoch: 380, loss 1469.7744621319648\n",
      "Epoch: 381, loss 1469.7744621319648\n",
      "Epoch: 382, loss 1469.7744621319648\n",
      "Epoch: 383, loss 1469.7744621319648\n",
      "Epoch: 384, loss 1469.7744621319648\n",
      "Epoch: 385, loss 1469.7744621319648\n",
      "Epoch: 386, loss 1469.7744621319648\n",
      "Epoch: 387, loss 1469.7744621319648\n",
      "Epoch: 388, loss 1469.7744621319648\n",
      "Epoch: 389, loss 1469.7744621319648\n",
      "Epoch: 390, loss 1469.7744621319648\n",
      "Epoch: 391, loss 1469.7744621319648\n",
      "Epoch: 392, loss 1469.7744621319648\n",
      "Epoch: 393, loss 1469.7744621319648\n",
      "Epoch: 394, loss 1469.7744621319648\n",
      "Epoch: 395, loss 1469.7744621319648\n",
      "Epoch: 396, loss 1469.7744621319648\n",
      "Epoch: 397, loss 1469.7744621319648\n",
      "Epoch: 398, loss 1469.7744621319648\n",
      "Epoch: 399, loss 1469.7744621319648\n",
      "Epoch: 400, loss 1469.7744621319648\n",
      "Epoch: 401, loss 1469.7744621319648\n",
      "Epoch: 402, loss 1469.7744621319648\n",
      "Epoch: 403, loss 1469.7744621319648\n",
      "Epoch: 404, loss 1469.7744621319648\n",
      "Epoch: 405, loss 1469.7744621319648\n",
      "Epoch: 406, loss 1469.7744621319648\n",
      "Epoch: 407, loss 1469.7744621319648\n",
      "Epoch: 408, loss 1469.7744621319648\n",
      "Epoch: 409, loss 1469.7744621319648\n",
      "Epoch: 410, loss 1469.7744621319648\n",
      "Epoch: 411, loss 1469.7744621319648\n",
      "Epoch: 412, loss 1469.7744621319648\n",
      "Epoch: 413, loss 1469.7744621319648\n",
      "Epoch: 414, loss 1469.7744621319648\n",
      "Epoch: 415, loss 1469.7744621319648\n",
      "Epoch: 416, loss 1469.7744621319648\n",
      "Epoch: 417, loss 1469.7744621319648\n",
      "Epoch: 418, loss 1469.7744621319648\n",
      "Epoch: 419, loss 1469.7744621319648\n",
      "Epoch: 420, loss 1469.7744621319648\n",
      "Epoch: 421, loss 1469.7744621319648\n",
      "Epoch: 422, loss 1469.7744621319648\n",
      "Epoch: 423, loss 1469.7744621319648\n",
      "Epoch: 424, loss 1469.7744621319648\n",
      "Epoch: 425, loss 1469.7744621319648\n",
      "Epoch: 426, loss 1469.7744621319648\n",
      "Epoch: 427, loss 1469.7744621319648\n",
      "Epoch: 428, loss 1469.7744621319648\n",
      "Epoch: 429, loss 1469.7744621319648\n",
      "Epoch: 430, loss 1469.7744621319648\n",
      "Epoch: 431, loss 1469.7744621319648\n",
      "Epoch: 432, loss 1469.7744621319648\n",
      "Epoch: 433, loss 1469.7744621319648\n",
      "Epoch: 434, loss 1469.7744621319648\n",
      "Epoch: 435, loss 1469.7744621319648\n",
      "Epoch: 436, loss 1469.7744621319648\n",
      "Epoch: 437, loss 1469.7744621319648\n",
      "Epoch: 438, loss 1469.7744621319648\n",
      "Epoch: 439, loss 1469.7744621319648\n",
      "Epoch: 440, loss 1469.7744621319648\n",
      "Epoch: 441, loss 1469.7744621319648\n",
      "Epoch: 442, loss 1469.7744621319648\n",
      "Epoch: 443, loss 1469.7744621319648\n",
      "Epoch: 444, loss 1469.7744621319648\n",
      "Epoch: 445, loss 1469.7744621319648\n",
      "Epoch: 446, loss 1469.7744621319648\n",
      "Epoch: 447, loss 1469.7744621319648\n",
      "Epoch: 448, loss 1469.7744621319648\n",
      "Epoch: 449, loss 1469.7744621319648\n",
      "Epoch: 450, loss 1469.7744621319648\n",
      "Epoch: 451, loss 1469.7744621319648\n",
      "Epoch: 452, loss 1469.7744621319648\n",
      "Epoch: 453, loss 1469.7744621319648\n",
      "Epoch: 454, loss 1469.7744621319648\n",
      "Epoch: 455, loss 1469.7744621319648\n",
      "Epoch: 456, loss 1469.7744621319648\n",
      "Epoch: 457, loss 1469.7744621319648\n",
      "Epoch: 458, loss 1469.7744621319648\n",
      "Epoch: 459, loss 1469.7744621319648\n",
      "Epoch: 460, loss 1469.7744621319648\n",
      "Epoch: 461, loss 1469.7744621319648\n",
      "Epoch: 462, loss 1469.7744621319648\n",
      "Epoch: 463, loss 1469.7744621319648\n",
      "Epoch: 464, loss 1469.7744621319648\n",
      "Epoch: 465, loss 1469.7744621319648\n",
      "Epoch: 466, loss 1469.7744621319648\n",
      "Epoch: 467, loss 1469.7744621319648\n",
      "Epoch: 468, loss 1469.7744621319648\n",
      "Epoch: 469, loss 1469.7744621319648\n",
      "Epoch: 470, loss 1469.7744621319648\n",
      "Epoch: 471, loss 1469.7744621319648\n",
      "Epoch: 472, loss 1469.7744621319648\n",
      "Epoch: 473, loss 1469.7744621319648\n",
      "Epoch: 474, loss 1469.7744621319648\n",
      "Epoch: 475, loss 1469.7744621319648\n",
      "Epoch: 476, loss 1469.7744621319648\n",
      "Epoch: 477, loss 1469.7744621319648\n",
      "Epoch: 478, loss 1469.7744621319648\n",
      "Epoch: 479, loss 1469.7744621319648\n",
      "Epoch: 480, loss 1469.7744621319648\n",
      "Epoch: 481, loss 1469.7744621319648\n",
      "Epoch: 482, loss 1469.7744621319648\n",
      "Epoch: 483, loss 1469.7744621319648\n",
      "Epoch: 484, loss 1469.7744621319648\n",
      "Epoch: 485, loss 1469.7744621319648\n",
      "Epoch: 486, loss 1469.7744621319648\n",
      "Epoch: 487, loss 1469.7744621319648\n",
      "Epoch: 488, loss 1469.7744621319648\n",
      "Epoch: 489, loss 1469.7744621319648\n",
      "Epoch: 490, loss 1469.7744621319648\n",
      "Epoch: 491, loss 1469.7744621319648\n",
      "Epoch: 492, loss 1469.7744621319648\n",
      "Epoch: 493, loss 1469.7744621319648\n",
      "Epoch: 494, loss 1469.7744621319648\n",
      "Epoch: 495, loss 1469.7744621319648\n",
      "Epoch: 496, loss 1469.7744621319648\n",
      "Epoch: 497, loss 1469.7744621319648\n",
      "Epoch: 498, loss 1469.7744621319648\n",
      "Epoch: 499, loss 1469.7744621319648\n",
      "Epoch: 500, loss 1469.7744621319648\n"
     ]
    }
   ],
   "source": [
    "# we'll start building our neural network training app here\n",
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1.5\n",
    "w2 = 0.5\n",
    "w3 = -2\n",
    "w4 = -0.5\n",
    "w5 = 1.5\n",
    "w6 = 1.2\n",
    "bias1 = 0.5\n",
    "bias2 = -0.35\n",
    "bias3 = 0.1\n",
    "\n",
    "# we'll save these for future\n",
    "# se we can compare results to the final weights\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    " \n",
    "# learning rate and epochs\n",
    "LR = 0.005\n",
    "epochs = 500\n",
    "\n",
    "# Regularization strength for both weights and biases\n",
    "regularization_strength = 0.025\n",
    "\n",
    "# override above data with our generation function\n",
    "data = generate_train_data()\n",
    "\n",
    "# let's initialize a list for loss points\n",
    "loss_points = []\n",
    "\n",
    "# START THE TRAINING PROCESS\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # let's also monitor epoch-wise losses\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # for example with first row\n",
    "        # [1, 0, 2] => assign input1 = 1, input2 = 0, true_value = 2\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # FORWARD PASS\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the the previous weights\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        # probably used later, we might want to have error metrics (MSE)\n",
    "        predicted_value = node_3_output\n",
    "\n",
    "        # LOSS CALCULATION WITH REGULARIZATION\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "        loss += regularization_strength * (w1**2 + w2**2 + w3**2 + w4**2 + w5**2 + w6**2)  # Weight decay\n",
    "        loss += regularization_strength * (bias1**2 + bias2**2 + bias3**2)  # Bias penalty\n",
    "      \n",
    "        # add current training data row loss to epoch losses\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solve w5 and update the new value\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        # solve w6 and update the new value\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        # solve bias3 and update the new value\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT FORWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES -> CHAIN RULE\n",
    "\n",
    "        # see the materials and the math experiment notebook for more details\n",
    "        # start with weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS!\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it the loss_points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the average loss of this epoch into the overall loss list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1.5\n",
      "w2: 0.5\n",
      "w3: -2\n",
      "w4: -0.5\n",
      "w5: 1.5\n",
      "w6: 1.2\n",
      "b1: 0.5\n",
      "b2: -0.35\n",
      "b3: 0.1\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: -22.57879330308162\n",
      "w2: 0.6993718191591087\n",
      "w3: -24.56428517171817\n",
      "w4: -7.2208363119949786\n",
      "w5: -125.79115347161417\n",
      "w6: -75.69495229790346\n",
      "b1: -2.7310162755341576\n",
      "b2: -0.7294230413757751\n",
      "b3: 39.238939622948\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # FORWARD PASS\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the the previous weights\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.8287387933988777, 7.268514535642031, 18.262314036672528]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.027077098262832, 11.128945257629677, 37.41257177726149]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.238939622948"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # the value tends to be same as final bias3 \n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.03846154, 0.23076923)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.238939622948"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the value tends to be same as final bias3 \n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "predict(1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39.238939622948"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(30,150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
