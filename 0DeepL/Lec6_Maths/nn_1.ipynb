{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss 0.25 \n",
      "Epoch: 1, loss 2.6386753600000015 \n",
      "Epoch: 1, loss 76.53988993500299 \n",
      "Epoch: 2, loss 29.29921894674407 \n",
      "Epoch: 2, loss 40.19348698346555 \n",
      "Epoch: 2, loss 30.137554813824483 \n",
      "Epoch: 3, loss 9.806898915080314 \n",
      "Epoch: 3, loss 27.71541794005157 \n",
      "Epoch: 3, loss 14.817266949082901 \n",
      "Epoch: 4, loss 3.3775663355556365 \n",
      "Epoch: 4, loss 20.073811286732774 \n",
      "Epoch: 4, loss 7.3265926946506985 \n",
      "Epoch: 5, loss 0.7256324669468154 \n",
      "Epoch: 5, loss 13.355531248122164 \n",
      "Epoch: 5, loss 3.4563896716042644 \n",
      "Epoch: 6, loss 0.018174252891030477 \n",
      "Epoch: 6, loss 8.465792593886501 \n",
      "Epoch: 6, loss 1.7664630113995352 \n",
      "Epoch: 7, loss 0.10973153419555766 \n",
      "Epoch: 7, loss 5.664875955165463 \n",
      "Epoch: 7, loss 1.0769770839610753 \n",
      "Epoch: 8, loss 0.37907960172494676 \n",
      "Epoch: 8, loss 4.229777347771538 \n",
      "Epoch: 8, loss 0.7561413943131118 \n",
      "Epoch: 9, loss 0.6299676589035117 \n",
      "Epoch: 9, loss 3.4407835047171 \n",
      "Epoch: 9, loss 0.5829062024193482 \n",
      "Epoch: 10, loss 0.8252060314165627 \n",
      "Epoch: 10, loss 2.963787500907885 \n",
      "Epoch: 10, loss 0.4819095273320173 \n",
      "Epoch: 11, loss 0.9651204864440052 \n",
      "Epoch: 11, loss 2.660900318012953 \n",
      "Epoch: 11, loss 0.4197125089920903 \n",
      "Epoch: 12, loss 1.0603002020319567 \n",
      "Epoch: 12, loss 2.461115372344453 \n",
      "Epoch: 12, loss 0.37969427125971045 \n",
      "Epoch: 13, loss 1.1222698801737094 \n",
      "Epoch: 13, loss 2.324716944637705 \n",
      "Epoch: 13, loss 0.3529363086757412 \n",
      "Epoch: 14, loss 1.1606601327195647 \n",
      "Epoch: 14, loss 2.2283201559585764 \n",
      "Epoch: 14, loss 0.33437352618824867 \n",
      "Epoch: 15, loss 1.1827394532474742 \n",
      "Epoch: 15, loss 2.1576572699072334 \n",
      "Epoch: 15, loss 0.32100273169524224 \n",
      "Epoch: 16, loss 1.1937206938777032 \n",
      "Epoch: 16, loss 2.103798368700222 \n",
      "Epoch: 16, loss 0.3109845398441827 \n",
      "Epoch: 17, loss 1.1972370169552116 \n",
      "Epoch: 17, loss 2.0610501387253586 \n",
      "Epoch: 17, loss 0.3031647792287023 \n",
      "Epoch: 18, loss 1.1957780744706523 \n",
      "Epoch: 18, loss 2.025733285638327 \n",
      "Epoch: 18, loss 0.29680613996189614 \n",
      "Epoch: 19, loss 1.1910324051936196 \n",
      "Epoch: 19, loss 1.9954459008459777 \n",
      "Epoch: 19, loss 0.2914312969918247 \n",
      "Epoch: 20, loss 1.1841379041519227 \n",
      "Epoch: 20, loss 1.9686076856818495 \n",
      "Epoch: 20, loss 0.286728107129284 \n",
      "Epoch: 21, loss 1.1758582658026975 \n",
      "Epoch: 21, loss 1.944172292848149 \n",
      "Epoch: 21, loss 0.2824908283883283 \n",
      "Epoch: 22, loss 1.1667046143013475 \n",
      "Epoch: 22, loss 1.9214430776811688 \n",
      "Epoch: 22, loss 0.27858295957151796 \n",
      "Epoch: 23, loss 1.1570182771677548 \n",
      "Epoch: 23, loss 1.8999537892152998 \n",
      "Epoch: 23, loss 0.2749134047969726 \n",
      "Epoch: 24, loss 1.147026685761996 \n",
      "Epoch: 24, loss 1.8793906805383278 \n",
      "Epoch: 24, loss 0.27142101916195843 \n",
      "Epoch: 25, loss 1.1368809592445412 \n",
      "Epoch: 25, loss 1.8595413443823654 \n",
      "Epoch: 25, loss 0.26806450739154203 \n",
      "Epoch: 26, loss 1.126681112169521 \n",
      "Epoch: 26, loss 1.84026094270918 \n",
      "Epoch: 26, loss 0.2648157808251466 \n",
      "Epoch: 27, loss 1.1164929418652987 \n",
      "Epoch: 27, loss 1.8214498315048189 \n",
      "Epoch: 27, loss 0.2616555679587877 \n",
      "Epoch: 28, loss 1.1063593381532717 \n",
      "Epoch: 28, loss 1.8030386886638863 \n",
      "Epoch: 28, loss 0.2585705030490135 \n",
      "Epoch: 29, loss 1.0963078590294921 \n",
      "Epoch: 29, loss 1.7849786020228342 \n",
      "Epoch: 29, loss 0.25555118901627805 \n",
      "Epoch: 30, loss 1.0863558076006627 \n",
      "Epoch: 30, loss 1.7672344471522 \n",
      "Epoch: 30, loss 0.2525909051253369 \n",
      "Epoch: 31, loss 1.0765136366001626 \n",
      "Epoch: 31, loss 1.7497804530234564 \n",
      "Epoch: 31, loss 0.2496847427419202 \n",
      "Epoch: 32, loss 1.0667872329115222 \n",
      "Epoch: 32, loss 1.732597226193745 \n",
      "Epoch: 32, loss 0.24682902606028062 \n",
      "Epoch: 33, loss 1.057179451450102 \n",
      "Epoch: 33, loss 1.7156697493556037 \n",
      "Epoch: 33, loss 0.24402092297777275 \n",
      "Epoch: 34, loss 1.047691145480979 \n",
      "Epoch: 34, loss 1.6989860320817116 \n",
      "Epoch: 34, loss 0.24125818310803582 \n",
      "Epoch: 35, loss 1.0383218587914216 \n",
      "Epoch: 35, loss 1.6825361989199024 \n",
      "Epoch: 35, loss 0.23853896096527719 \n",
      "Epoch: 36, loss 1.0290702905742568 \n",
      "Epoch: 36, loss 1.6663118712882634 \n",
      "Epoch: 36, loss 0.23586169630772977 \n",
      "Epoch: 37, loss 1.0199346073939515 \n",
      "Epoch: 37, loss 1.6503057470839428 \n",
      "Epoch: 37, loss 0.23322503290777802 \n",
      "Epoch: 38, loss 1.0109126521869287 \n",
      "Epoch: 38, loss 1.634511313580582 \n",
      "Epoch: 38, loss 0.23062776320077666 \n",
      "Epoch: 39, loss 1.0020020838830839 \n",
      "Epoch: 39, loss 1.6189226503516563 \n",
      "Epoch: 39, loss 0.22806879039233602 \n",
      "Epoch: 40, loss 0.9932004702585018 \n",
      "Epoch: 40, loss 1.6035342931187648 \n",
      "Epoch: 40, loss 0.22554710236661094 \n",
      "Epoch: 41, loss 0.9845053492545672 \n",
      "Epoch: 41, loss 1.588341138925146 \n",
      "Epoch: 41, loss 0.22306175358700553 \n",
      "Epoch: 42, loss 0.9759142690410983 \n",
      "Epoch: 42, loss 1.573338379408855 \n",
      "Epoch: 42, loss 0.22061185242316778 \n",
      "Epoch: 43, loss 0.967424813761532 \n",
      "Epoch: 43, loss 1.5585214532420513 \n",
      "Epoch: 43, loss 0.2181965521714419 \n",
      "Epoch: 44, loss 0.9590346196484822 \n",
      "Epoch: 44, loss 1.5438860116888236 \n",
      "Epoch: 44, loss 0.21581504459776257 \n",
      "Epoch: 45, loss 0.9507413846789141 \n",
      "Epoch: 45, loss 1.5294278931832317 \n",
      "Epoch: 45, loss 0.2134665552099052 \n",
      "Epoch: 46, loss 0.9425428739125352 \n",
      "Epoch: 46, loss 1.5151431041435517 \n",
      "Epoch: 46, loss 0.21115033972130964 \n",
      "Epoch: 47, loss 0.9344369219631825 \n",
      "Epoch: 47, loss 1.5010278041287761 \n",
      "Epoch: 47, loss 0.20886568134088085 \n",
      "Epoch: 48, loss 0.9264214335837078 \n",
      "Epoch: 48, loss 1.4870782940447849 \n",
      "Epoch: 48, loss 0.206611888640185 \n",
      "Epoch: 49, loss 0.9184943830265132 \n",
      "Epoch: 49, loss 1.4732910065172073 \n",
      "Epoch: 49, loss 0.2043882938278264 \n",
      "Epoch: 50, loss 0.9106538126269182 \n",
      "Epoch: 50, loss 1.4596624978234476 \n",
      "Epoch: 50, loss 0.20219425131513136 \n"
     ]
    }
   ],
   "source": [
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "orignal_w1 = w1\n",
    "orignal_w2 = w2\n",
    "orignal_w3 = w3\n",
    "orignal_w4 = w4\n",
    "orignal_w5 = w5\n",
    "orignal_w6 = w6\n",
    "orignal_b1 = bias1\n",
    "orignal_b2 = bias2\n",
    "orignal_b3 = bias3\n",
    "\n",
    "LR=0.01\n",
    "epochs = 50\n",
    "\n",
    "data= [\n",
    "    [1,0,2],\n",
    "    [2,1,6],\n",
    "    [3,3,17]\n",
    "]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for row in data:\n",
    "        input1=row[0]\n",
    "        input2=row[1]\n",
    "        true_value = row[2]\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "        node_3_output = node_1_output * w5 + node_2_output*w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "        # #loss\n",
    "        # #Back\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "        print(f\"Epoch: {epoch +1}, loss: {loss} \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
