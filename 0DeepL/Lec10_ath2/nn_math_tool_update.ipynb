{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural network, experimentation tool, version 1</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# just copy/paste -the needed activation functions, \n",
    "# we're going to need these again\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lock down the randomness\n",
    "# np.random.seed(123)\n",
    "\n",
    "def generate_train_data():\n",
    "    result = []\n",
    "\n",
    "    # create 100 numbers\n",
    "    for x in range(100):\n",
    "        n1 = np.random.randint(0, 5)\n",
    "        n2 = np.random.randint(3, 7)\n",
    "\n",
    "        # formula for target variable: x1 ^^ 2 + x2 + (random integer between 0-5)\n",
    "        n3 = n1 ** 2 + n2 + np.random.randint(0, 5)\n",
    "        n3 = int(n3)\n",
    "\n",
    "        # add the row to result (y is target)\n",
    "        # basically the order of variables: x1, x2, y \n",
    "        result.append([n1, n2, n3])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override above data with our generation function\n",
    "data = generate_train_data()\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"x1\", \"x2\", \"y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The neural network training code</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss 0.1042012693402651\n",
      "Epoch: 2, loss 0.060052223679302075\n",
      "Epoch: 3, loss 0.05204148105239935\n",
      "Epoch: 4, loss 0.05057116901174413\n",
      "Epoch: 5, loss 0.04927524482641086\n",
      "Epoch: 6, loss 0.04814260077396988\n",
      "Epoch: 7, loss 0.04667260994253158\n",
      "Epoch: 8, loss 0.04515615898311509\n",
      "Epoch: 9, loss 0.043905054347424435\n",
      "Epoch: 10, loss 0.042289308851065986\n",
      "Epoch: 11, loss 0.04109294034500854\n",
      "Epoch: 12, loss 0.03992967537831297\n",
      "Epoch: 13, loss 0.03878056091334399\n",
      "Epoch: 14, loss 0.03764505944064704\n",
      "Epoch: 15, loss 0.0365235332096784\n",
      "Epoch: 16, loss 0.035416381152502646\n",
      "Epoch: 17, loss 0.034323996213958353\n",
      "Epoch: 18, loss 0.03324676219617534\n",
      "Epoch: 19, loss 0.03218505249306072\n",
      "Epoch: 20, loss 0.031139228948490593\n",
      "Epoch: 21, loss 0.03010964076115671\n",
      "Epoch: 22, loss 0.029096623436565192\n",
      "Epoch: 23, loss 0.028100497789994468\n",
      "Epoch: 24, loss 0.027121569004157593\n",
      "Epoch: 25, loss 0.02616012574508185\n",
      "Epoch: 26, loss 0.025216439339444327\n",
      "Epoch: 27, loss 0.024290763016310873\n",
      "Epoch: 28, loss 0.0233833312159028\n",
      "Epoch: 29, loss 0.02249435896767677\n",
      "Epoch: 30, loss 0.021624041339649222\n",
      "Epoch: 31, loss 0.02077255296052014\n",
      "Epoch: 32, loss 0.019940047615774108\n",
      "Epoch: 33, loss 0.019126657918543795\n",
      "Epoch: 34, loss 0.01833249505562812\n",
      "Epoch: 35, loss 0.01755764860866511\n",
      "Epoch: 36, loss 0.016802186450066807\n",
      "Epoch: 37, loss 0.016066154712940012\n",
      "Epoch: 38, loss 0.015349577833841986\n",
      "Epoch: 39, loss 0.014652458666858059\n",
      "Epoch: 40, loss 0.013974778667140995\n",
      "Epoch: 41, loss 0.01331649814172574\n",
      "Epoch: 42, loss 0.012864958500476014\n",
      "Epoch: 43, loss 0.012072254365536515\n",
      "Epoch: 44, loss 0.011642113591799645\n",
      "Epoch: 45, loss 0.010898730486077486\n",
      "Epoch: 46, loss 0.010493718957864034\n",
      "Epoch: 47, loss 0.009956234683472603\n",
      "Epoch: 48, loss 0.009281487536938563\n",
      "Epoch: 49, loss 0.008914208054801026\n",
      "Epoch: 50, loss 0.008429575011431949\n",
      "Epoch: 51, loss 0.00795961131790475\n",
      "Epoch: 52, loss 0.00737753441355468\n",
      "Epoch: 53, loss 0.007058828360523029\n",
      "Epoch: 54, loss 0.0066419072639604115\n",
      "Epoch: 55, loss 0.0062393654515673705\n",
      "Epoch: 56, loss 0.005852899020502688\n",
      "Epoch: 57, loss 0.005482281503827813\n",
      "Epoch: 58, loss 0.005127242251914084\n",
      "Epoch: 59, loss 0.0047874988603338565\n",
      "Epoch: 60, loss 0.004462758670751352\n",
      "Epoch: 61, loss 0.0041527197539199725\n",
      "Epoch: 62, loss 0.0038570718691973184\n",
      "Epoch: 63, loss 0.0035754974057875593\n",
      "Epoch: 64, loss 0.0033076723033239877\n",
      "Epoch: 65, loss 0.0030532669495054164\n",
      "Epoch: 66, loss 0.0028119470527191268\n",
      "Epoch: 67, loss 0.0025833744878048398\n",
      "Epoch: 68, loss 0.0023672081133306205\n",
      "Epoch: 69, loss 0.002163104558965517\n",
      "Epoch: 70, loss 0.001970718981742644\n",
      "Epoch: 71, loss 0.0017897057902081443\n",
      "Epoch: 72, loss 0.0016197193356476429\n",
      "Epoch: 73, loss 0.0014281866397814653\n",
      "Epoch: 74, loss 0.0012731107717688158\n",
      "Epoch: 75, loss 0.0011387366821787126\n",
      "Epoch: 76, loss 0.0010109985774842062\n",
      "Epoch: 77, loss 0.000885282078784216\n",
      "Epoch: 78, loss 0.0007714605742422634\n",
      "Epoch: 79, loss 0.000666882433514906\n",
      "Epoch: 80, loss 0.0005867971922749693\n",
      "Epoch: 81, loss 0.0004981420674388551\n",
      "Epoch: 82, loss 0.00041747624784739247\n",
      "Epoch: 83, loss 0.00034476409789313154\n",
      "Epoch: 84, loss 0.00027976883387340436\n",
      "Epoch: 85, loss 0.00022224705747371526\n",
      "Epoch: 86, loss 0.0001719524693908904\n",
      "Epoch: 87, loss 0.00012863719432703067\n",
      "Epoch: 88, loss 9.205289926498605e-05\n",
      "Epoch: 89, loss 6.195175351739212e-05\n",
      "Epoch: 90, loss 3.808725192208362e-05\n",
      "Epoch: 91, loss 1.9060115044397142e-05\n",
      "Epoch: 92, loss 6.760565884486174e-06\n",
      "Epoch: 93, loss 7.219544017422054e-07\n",
      "Epoch: 94, loss 7.705364621608286e-07\n",
      "Epoch: 95, loss 6.672171407885956e-06\n",
      "Epoch: 96, loss 1.8205224808581025e-05\n",
      "Epoch: 97, loss 3.5862059401333404e-05\n",
      "Epoch: 98, loss 5.454551157424621e-05\n",
      "Epoch: 99, loss 7.975531032710974e-05\n",
      "Epoch: 100, loss 9.895877705232165e-05\n",
      "Epoch: 101, loss 0.00013961399113786294\n",
      "Epoch: 102, loss 0.0001664318121546798\n",
      "Epoch: 103, loss 0.00020342598824349596\n",
      "Epoch: 104, loss 0.0002443188395611319\n",
      "Epoch: 105, loss 0.00030210391096520146\n",
      "Epoch: 106, loss 0.0003373927999192449\n",
      "Epoch: 107, loss 0.00038604072836062744\n",
      "Epoch: 108, loss 0.00043831826051348957\n",
      "Epoch: 109, loss 0.0004939463381585418\n",
      "Epoch: 110, loss 0.000528309623192574\n",
      "Epoch: 111, loss 0.0005453961076755274\n",
      "Epoch: 112, loss 0.000582429083125809\n",
      "Epoch: 113, loss 0.0006390749843807709\n",
      "Epoch: 114, loss 0.0006869588507353747\n",
      "Epoch: 115, loss 0.0007267862214484877\n",
      "Epoch: 116, loss 0.000760261415393126\n",
      "Epoch: 117, loss 0.0007888040084859117\n",
      "Epoch: 118, loss 0.000813548263949336\n",
      "Epoch: 119, loss 0.0008353814780306959\n",
      "Epoch: 120, loss 0.0008549878043880289\n",
      "Epoch: 121, loss 0.0008728893557991807\n",
      "Epoch: 122, loss 0.0008894815572187772\n",
      "Epoch: 123, loss 0.0009050621371890732\n",
      "Epoch: 124, loss 0.0009198542158478459\n",
      "Epoch: 125, loss 0.0009340243419188454\n",
      "Epoch: 126, loss 0.0009476963995544395\n",
      "Epoch: 127, loss 0.0009609622313215076\n",
      "Epoch: 128, loss 0.0009738896978955725\n",
      "Epoch: 129, loss 0.0009865287620821853\n",
      "Epoch: 130, loss 0.0009989160636854684\n",
      "Epoch: 131, loss 0.001011078349099291\n",
      "Epoch: 132, loss 0.0010230350359956386\n",
      "Epoch: 133, loss 0.0010348001272807625\n",
      "Epoch: 134, loss 0.0010463836368861431\n",
      "Epoch: 135, loss 0.0010549766962097101\n",
      "Epoch: 136, loss 0.001064044121838713\n",
      "Epoch: 137, loss 0.0010735742081409267\n",
      "Epoch: 138, loss 0.0010780245840351897\n",
      "Epoch: 139, loss 0.001067037465780875\n",
      "Epoch: 140, loss 0.0010622720802391412\n",
      "Epoch: 141, loss 0.0010620749919138858\n",
      "Epoch: 142, loss 0.0010651026195750454\n",
      "Epoch: 143, loss 0.001116123763919266\n",
      "Epoch: 144, loss 0.001137504361892651\n",
      "Epoch: 145, loss 0.001176072314668636\n",
      "Epoch: 146, loss 0.0012345485019243374\n",
      "Epoch: 147, loss 0.0012803749004893482\n",
      "Epoch: 148, loss 0.0013165538545175583\n",
      "Epoch: 149, loss 0.0013456513754471673\n",
      "Epoch: 150, loss 0.0013695773164640948\n",
      "Epoch: 151, loss 0.0013897325330364482\n",
      "Epoch: 152, loss 0.0014071353592697402\n",
      "Epoch: 153, loss 0.0014225214265863886\n",
      "Epoch: 154, loss 0.0014364197613398583\n",
      "Epoch: 155, loss 0.0014492095656691217\n",
      "Epoch: 156, loss 0.0014611619915504733\n",
      "Epoch: 157, loss 0.0014724705809196171\n",
      "Epoch: 158, loss 0.0014832732988211816\n",
      "Epoch: 159, loss 0.0014936684051417716\n",
      "Epoch: 160, loss 0.0015037258478042027\n",
      "Epoch: 161, loss 0.0015134954195376183\n",
      "Epoch: 162, loss 0.0015510716662710442\n",
      "Epoch: 163, loss 0.0015846080548113342\n",
      "Epoch: 164, loss 0.0016108457654012394\n",
      "Epoch: 165, loss 0.001631815228292379\n",
      "Epoch: 166, loss 0.0016490148625595029\n",
      "Epoch: 167, loss 0.0016635122396451182\n",
      "Epoch: 168, loss 0.001676063942756733\n",
      "Epoch: 169, loss 0.001687204300520336\n",
      "Epoch: 170, loss 0.0016973097562863098\n",
      "Epoch: 171, loss 0.0017066450724646316\n",
      "Epoch: 172, loss 0.0017153962585340734\n",
      "Epoch: 173, loss 0.0017236939196727213\n",
      "Epoch: 174, loss 0.001718952326875164\n",
      "Epoch: 175, loss 0.001717615388665651\n",
      "Epoch: 176, loss 0.0017053720475819964\n",
      "Epoch: 177, loss 0.0016988256693525575\n",
      "Epoch: 178, loss 0.0016868487797101971\n",
      "Epoch: 179, loss 0.0016776845799539955\n",
      "Epoch: 180, loss 0.0016736424140530442\n",
      "Epoch: 181, loss 0.0016730422165667125\n",
      "Epoch: 182, loss 0.0017063974976493641\n",
      "Epoch: 183, loss 0.0017357566766817518\n",
      "Epoch: 184, loss 0.001753850120883857\n",
      "Epoch: 185, loss 0.0018036926771940243\n",
      "Epoch: 186, loss 0.001839685114037567\n",
      "Epoch: 187, loss 0.0018658313877941996\n",
      "Epoch: 188, loss 0.0018852598843096863\n",
      "Epoch: 189, loss 0.0019001174916097516\n",
      "Epoch: 190, loss 0.0019118642643116918\n",
      "Epoch: 191, loss 0.0019214882084493063\n",
      "Epoch: 192, loss 0.0019296556525968647\n",
      "Epoch: 193, loss 0.0019368149517052745\n",
      "Epoch: 194, loss 0.0019432673271742972\n",
      "Epoch: 195, loss 0.0019492149528225334\n",
      "Epoch: 196, loss 0.0019547934557963198\n",
      "Epoch: 197, loss 0.0019600938182459436\n",
      "Epoch: 198, loss 0.001965177104953418\n",
      "Epoch: 199, loss 0.0019700843512231434\n",
      "Epoch: 200, loss 0.001974843193426374\n",
      "Epoch: 201, loss 0.0019794723109595367\n",
      "Epoch: 202, loss 0.0019839843996110377\n",
      "Epoch: 203, loss 0.0019883881604762996\n",
      "Epoch: 204, loss 0.0019926896295118742\n",
      "Epoch: 205, loss 0.0019968930657834126\n",
      "Epoch: 206, loss 0.002001001544539722\n",
      "Epoch: 207, loss 0.0020050173529703053\n",
      "Epoch: 208, loss 0.0020089422541348136\n",
      "Epoch: 209, loss 0.0020127776628639815\n",
      "Epoch: 210, loss 0.0020165247629093147\n",
      "Epoch: 211, loss 0.0019817480173528845\n",
      "Epoch: 212, loss 0.0019592089556374967\n",
      "Epoch: 213, loss 0.001945805357115076\n",
      "Epoch: 214, loss 0.0019301996293190763\n",
      "Epoch: 215, loss 0.001921170587162776\n",
      "Epoch: 216, loss 0.0019165747040357365\n",
      "Epoch: 217, loss 0.0019148496705999858\n",
      "Epoch: 218, loss 0.0019149769590063915\n",
      "Epoch: 219, loss 0.001916292185649888\n",
      "Epoch: 220, loss 0.0019183614782319178\n",
      "Epoch: 221, loss 0.0019209013561586681\n",
      "Epoch: 222, loss 0.0019237265990247167\n",
      "Epoch: 223, loss 0.0019267162361453973\n",
      "Epoch: 224, loss 0.0019297913228711562\n",
      "Epoch: 225, loss 0.0019329004059573246\n",
      "Epoch: 226, loss 0.0019360100137155397\n",
      "Epoch: 227, loss 0.0019390984331277737\n",
      "Epoch: 228, loss 0.00194215163818377\n",
      "Epoch: 229, loss 0.0019451606263530203\n",
      "Epoch: 230, loss 0.0019481196767291104\n",
      "Epoch: 231, loss 0.0019510252113167906\n",
      "Epoch: 232, loss 0.0019538750508997717\n",
      "Epoch: 233, loss 0.00195666792895391\n",
      "Epoch: 234, loss 0.0020328086198772573\n",
      "Epoch: 235, loss 0.002016270144137008\n",
      "Epoch: 236, loss 0.002073384026711226\n",
      "Epoch: 237, loss 0.0020435544146727557\n",
      "Epoch: 238, loss 0.0020926897160162047\n",
      "Epoch: 239, loss 0.002056913475496249\n",
      "Epoch: 240, loss 0.0021028654811369473\n",
      "Epoch: 241, loss 0.0021400214004224554\n",
      "Epoch: 242, loss 0.0021644633489020944\n",
      "Epoch: 243, loss 0.002180532930303855\n",
      "Epoch: 244, loss 0.0021911489665325454\n",
      "Epoch: 245, loss 0.0021982180326697767\n",
      "Epoch: 246, loss 0.00220297904495181\n",
      "Epoch: 247, loss 0.0022062344815659473\n",
      "Epoch: 248, loss 0.0022085030275135493\n",
      "Epoch: 249, loss 0.002210119470673183\n",
      "Epoch: 250, loss 0.002211299713555937\n",
      "Epoch: 251, loss 0.0022121829277331026\n",
      "Epoch: 252, loss 0.0022128588197825195\n",
      "Epoch: 253, loss 0.0022133852368988786\n",
      "Epoch: 254, loss 0.002213799520727054\n",
      "Epoch: 255, loss 0.0022394122883329934\n",
      "Epoch: 256, loss 0.0022314509263457604\n",
      "Epoch: 257, loss 0.002250651099096794\n",
      "Epoch: 258, loss 0.0022637965560438843\n",
      "Epoch: 259, loss 0.002271992206859803\n",
      "Epoch: 260, loss 0.002276992977405312\n",
      "Epoch: 261, loss 0.0022799362379331252\n",
      "Epoch: 262, loss 0.002281551204503914\n",
      "Epoch: 263, loss 0.0022823045099930575\n",
      "Epoch: 264, loss 0.00228249440890605\n",
      "Epoch: 265, loss 0.0022823114024314133\n",
      "Epoch: 266, loss 0.0022818771320625084\n",
      "Epoch: 267, loss 0.002281269279355665\n",
      "Epoch: 268, loss 0.0022724735578993926\n",
      "Epoch: 269, loss 0.002266312200219199\n",
      "Epoch: 270, loss 0.0022620760873040087\n",
      "Epoch: 271, loss 0.002259045955083526\n",
      "Epoch: 272, loss 0.002256764650451964\n",
      "Epoch: 273, loss 0.002254943024024341\n",
      "Epoch: 274, loss 0.002253398221835138\n",
      "Epoch: 275, loss 0.002252014670375234\n",
      "Epoch: 276, loss 0.002250719392150565\n",
      "Epoch: 277, loss 0.002249466377971663\n",
      "Epoch: 278, loss 0.002248226690288579\n",
      "Epoch: 279, loss 0.0022469821955950283\n",
      "Epoch: 280, loss 0.0022457215955629206\n",
      "Epoch: 281, loss 0.002244437914324556\n",
      "Epoch: 282, loss 0.00224312690807101\n",
      "Epoch: 283, loss 0.0022417860587252737\n",
      "Epoch: 284, loss 0.002240413937396152\n",
      "Epoch: 285, loss 0.002239009801872245\n",
      "Epoch: 286, loss 0.002237573342196087\n",
      "Epoch: 287, loss 0.0022361045198985995\n",
      "Epoch: 288, loss 0.0022346034664531915\n",
      "Epoch: 289, loss 0.002233070419159903\n",
      "Epoch: 290, loss 0.00223150568067922\n",
      "Epoch: 291, loss 0.002251589972265493\n",
      "Epoch: 292, loss 0.0022492052711928483\n",
      "Epoch: 293, loss 0.002261435632627679\n",
      "Epoch: 294, loss 0.0022754455861956017\n",
      "Epoch: 295, loss 0.002283437366613011\n",
      "Epoch: 296, loss 0.0022875797514594937\n",
      "Epoch: 297, loss 0.0022893016065435785\n",
      "Epoch: 298, loss 0.0022895005283333878\n",
      "Epoch: 299, loss 0.0022887388557558718\n",
      "Epoch: 300, loss 0.0022873684171999986\n",
      "Epoch: 301, loss 0.0022856091543832836\n",
      "Epoch: 302, loss 0.002283598504179709\n",
      "Epoch: 303, loss 0.002281422344728194\n",
      "Epoch: 304, loss 0.002279134359012004\n",
      "Epoch: 305, loss 0.0022767681380594287\n",
      "Epoch: 306, loss 0.002274344740049235\n",
      "Epoch: 307, loss 0.002271877408456072\n",
      "Epoch: 308, loss 0.0022693745154545634\n",
      "Epoch: 309, loss 0.0022668413973401845\n",
      "Epoch: 310, loss 0.0022642814985859407\n",
      "Epoch: 311, loss 0.002261697084691443\n",
      "Epoch: 312, loss 0.00225908968619521\n",
      "Epoch: 313, loss 0.0022564603751432026\n",
      "Epoch: 314, loss 0.0022538099371795274\n",
      "Epoch: 315, loss 0.0022511389786333802\n",
      "Epoch: 316, loss 0.0022484479931375667\n",
      "Epoch: 317, loss 0.0022457374030614855\n",
      "Epoch: 318, loss 0.0022430075852755554\n",
      "Epoch: 319, loss 0.002240258887170647\n",
      "Epoch: 320, loss 0.0022374916366189957\n",
      "Epoch: 321, loss 0.0022347061481696037\n",
      "Epoch: 322, loss 0.0022319027269041237\n",
      "Epoch: 323, loss 0.0022290816708394985\n",
      "Epoch: 324, loss 0.002226243272428598\n",
      "Epoch: 325, loss 0.002223387819500667\n",
      "Epoch: 326, loss 0.002220515595854147\n",
      "Epoch: 327, loss 0.002217626881634242\n",
      "Epoch: 328, loss 0.0022147219535764745\n",
      "Epoch: 329, loss 0.002211801085167451\n",
      "Epoch: 330, loss 0.002208864546754372\n",
      "Epoch: 331, loss 0.0022059126056224675\n",
      "Epoch: 332, loss 0.00220294552605286\n",
      "Epoch: 333, loss 0.002199963569367905\n",
      "Epoch: 334, loss 0.0021969669939692666\n",
      "Epoch: 335, loss 0.002193956055370921\n",
      "Epoch: 336, loss 0.002190931006229437\n",
      "Epoch: 337, loss 0.002187892096372269\n",
      "Epoch: 338, loss 0.002184839572824702\n",
      "Epoch: 339, loss 0.0021817736798364603\n",
      "Epoch: 340, loss 0.002178694658907283\n",
      "Epoch: 341, loss 0.0021756027488125476\n",
      "Epoch: 342, loss 0.0021724981856282985\n",
      "Epoch: 343, loss 0.0021693812027564537\n",
      "Epoch: 344, loss 0.0021662520309493577\n",
      "Epoch: 345, loss 0.0021631108983346495\n",
      "Epoch: 346, loss 0.0021599580304396972\n",
      "Epoch: 347, loss 0.002156793650215977\n",
      "Epoch: 348, loss 0.0021536179780634285\n",
      "Epoch: 349, loss 0.00215043123185438\n",
      "Epoch: 350, loss 0.0021472336269577614\n",
      "Epoch: 351, loss 0.0021440253762628728\n",
      "Epoch: 352, loss 0.002140806690203127\n",
      "Epoch: 353, loss 0.0021375777767796496\n",
      "Epoch: 354, loss 0.0021343388415847745\n",
      "Epoch: 355, loss 0.0021310900878253147\n",
      "Epoch: 356, loss 0.002127831716345857\n",
      "Epoch: 357, loss 0.00212456392565164\n",
      "Epoch: 358, loss 0.002121286911931653\n",
      "Epoch: 359, loss 0.0021180008690812714\n",
      "Epoch: 360, loss 0.002114705988724962\n",
      "Epoch: 361, loss 0.0021114024602386532\n",
      "Epoch: 362, loss 0.0021080904707721877\n",
      "Epoch: 363, loss 0.0021047702052713717\n",
      "Epoch: 364, loss 0.002101441846500103\n",
      "Epoch: 365, loss 0.0020981055750622763\n",
      "Epoch: 366, loss 0.002094761569423349\n",
      "Epoch: 367, loss 0.0020914100059321104\n",
      "Epoch: 368, loss 0.0020880510588419964\n",
      "Epoch: 369, loss 0.0020846849003323537\n",
      "Epoch: 370, loss 0.0020813117005295916\n",
      "Epoch: 371, loss 0.0020779316275281005\n",
      "Epoch: 372, loss 0.0020745448474112243\n",
      "Epoch: 373, loss 0.002071151524271627\n",
      "Epoch: 374, loss 0.0020677518202320442\n",
      "Epoch: 375, loss 0.002064345895465477\n",
      "Epoch: 376, loss 0.00206093390821544\n",
      "Epoch: 377, loss 0.0020575160148158965\n",
      "Epoch: 378, loss 0.0020540923697112603\n",
      "Epoch: 379, loss 0.002050663125476011\n",
      "Epoch: 380, loss 0.002047228432834268\n",
      "Epoch: 381, loss 0.0020437884406792188\n",
      "Epoch: 382, loss 0.0020403432960923286\n",
      "Epoch: 383, loss 0.0020368931443623894\n",
      "Epoch: 384, loss 0.0020334381290045326\n",
      "Epoch: 385, loss 0.0020299783917789183\n",
      "Epoch: 386, loss 0.002026514072709365\n",
      "Epoch: 387, loss 0.0020230453101018186\n",
      "Epoch: 388, loss 0.0020195722405627085\n",
      "Epoch: 389, loss 0.0020160949990169264\n",
      "Epoch: 390, loss 0.002012613718725979\n",
      "Epoch: 391, loss 0.0020091285313057422\n",
      "Epoch: 392, loss 0.0020056395667441425\n",
      "Epoch: 393, loss 0.0020548806940021065\n",
      "Epoch: 394, loss 0.0020306773043981563\n",
      "Epoch: 395, loss 0.002013978774564086\n",
      "Epoch: 396, loss 0.002055040101032675\n",
      "Epoch: 397, loss 0.002026041051065341\n",
      "Epoch: 398, loss 0.002006486953407767\n",
      "Epoch: 399, loss 0.002045608881828499\n",
      "Epoch: 400, loss 0.002015632612801968\n",
      "Epoch: 401, loss 0.0020476801115578776\n",
      "Epoch: 402, loss 0.0020135526675805955\n",
      "Epoch: 403, loss 0.002042972075460822\n",
      "Epoch: 404, loss 0.002007405765904984\n",
      "Epoch: 405, loss 0.002035822430520984\n",
      "Epoch: 406, loss 0.0019998034920233124\n",
      "Epoch: 407, loss 0.002027807116529513\n",
      "Epoch: 408, loss 0.0020433106468030833\n",
      "Epoch: 409, loss 0.001999073173631748\n",
      "Epoch: 410, loss 0.002021951171990426\n",
      "Epoch: 411, loss 0.0020343535783505884\n",
      "Epoch: 412, loss 0.0020398015469055212\n",
      "Epoch: 413, loss 0.001989876984560379\n",
      "Epoch: 414, loss 0.0020090757722031243\n",
      "Epoch: 415, loss 0.0020192527288708456\n",
      "Epoch: 416, loss 0.002023374039139156\n",
      "Epoch: 417, loss 0.0020238654541692765\n",
      "Epoch: 418, loss 0.00202219170594474\n",
      "Epoch: 419, loss 0.0020192309956937283\n",
      "Epoch: 420, loss 0.0020155087039010885\n",
      "Epoch: 421, loss 0.0020113386902798226\n",
      "Epoch: 422, loss 0.002006908261484021\n",
      "Epoch: 423, loss 0.0020023291061493083\n",
      "Epoch: 424, loss 0.0019976677694415607\n",
      "Epoch: 425, loss 0.001992963863208321\n",
      "Epoch: 426, loss 0.0019882409382916224\n",
      "Epoch: 427, loss 0.001983512971877478\n",
      "Epoch: 428, loss 0.0019787882366140833\n",
      "Epoch: 429, loss 0.0019740716073223985\n",
      "Epoch: 430, loss 0.001969365935773576\n",
      "Epoch: 431, loss 0.001964672869799476\n",
      "Epoch: 432, loss 0.0019599933411917894\n",
      "Epoch: 433, loss 0.0019553278562365398\n",
      "Epoch: 434, loss 0.0019506766686736832\n",
      "Epoch: 435, loss 0.00194603988263327\n",
      "Epoch: 436, loss 0.0019414175138784656\n",
      "Epoch: 437, loss 0.0019368095262304352\n",
      "Epoch: 438, loss 0.0019322158532236807\n",
      "Epoch: 439, loss 0.0019276364109741633\n",
      "Epoch: 440, loss 0.0019230711058206597\n",
      "Epoch: 441, loss 0.0019185198388580419\n",
      "Epoch: 442, loss 0.0019139825086229333\n",
      "Epoch: 443, loss 0.0019094590126812372\n",
      "Epoch: 444, loss 0.0019049492485631921\n",
      "Epoch: 445, loss 0.001900453114311016\n",
      "Epoch: 446, loss 0.0018959705087965432\n",
      "Epoch: 447, loss 0.001891501331902233\n",
      "Epoch: 448, loss 0.00188704548462147\n",
      "Epoch: 449, loss 0.0018826028691108373\n",
      "Epoch: 450, loss 0.0018781733887140444\n",
      "Epoch: 451, loss 0.0018737569479692442\n",
      "Epoch: 452, loss 0.001869353452606412\n",
      "Epoch: 453, loss 0.0018649628095391774\n",
      "Epoch: 454, loss 0.0018605849268532523\n",
      "Epoch: 455, loss 0.0018562197137930304\n",
      "Epoch: 456, loss 0.0018518670807472174\n",
      "Epoch: 457, loss 0.001847526939233916\n",
      "Epoch: 458, loss 0.0018431992018855452\n",
      "Epoch: 459, loss 0.0018388837824337063\n",
      "Epoch: 460, loss 0.0018345805956941426\n",
      "Epoch: 461, loss 0.0018302895575518139\n",
      "Epoch: 462, loss 0.0018260105849460966\n",
      "Epoch: 463, loss 0.0018217435958563274\n",
      "Epoch: 464, loss 0.0018174885092873175\n",
      "Epoch: 465, loss 0.00181324524525515\n",
      "Epoch: 466, loss 0.0018090137247733149\n",
      "Epoch: 467, loss 0.001804793869838802\n",
      "Epoch: 468, loss 0.0018005856034185822\n",
      "Epoch: 469, loss 0.001796388849436145\n",
      "Epoch: 470, loss 0.0017922035327583657\n",
      "Epoch: 471, loss 0.0017880295791824216\n",
      "Epoch: 472, loss 0.0017838669154230128\n",
      "Epoch: 473, loss 0.001779715469099699\n",
      "Epoch: 474, loss 0.0017755751687244209\n",
      "Epoch: 475, loss 0.0017714459436892099\n",
      "Epoch: 476, loss 0.0017673277242540822\n",
      "Epoch: 477, loss 0.0017632204415351195\n",
      "Epoch: 478, loss 0.0017591240274926593\n",
      "Epoch: 479, loss 0.0017550384149197106\n",
      "Epoch: 480, loss 0.0017509635374305624\n",
      "Epoch: 481, loss 0.0017468993294493898\n",
      "Epoch: 482, loss 0.0017428457261992826\n",
      "Epoch: 483, loss 0.0017388026636911483\n",
      "Epoch: 484, loss 0.0017347700787130557\n",
      "Epoch: 485, loss 0.0017307479088194354\n",
      "Epoch: 486, loss 0.0017267360923207298\n",
      "Epoch: 487, loss 0.0017227345682729153\n",
      "Epoch: 488, loss 0.0017187432764673796\n",
      "Epoch: 489, loss 0.001714762157420835\n",
      "Epoch: 490, loss 0.0017107911523653835\n",
      "Epoch: 491, loss 0.00170683020323876\n",
      "Epoch: 492, loss 0.0017028792526747032\n",
      "Epoch: 493, loss 0.001698938243993435\n",
      "Epoch: 494, loss 0.001695007121192257\n",
      "Epoch: 495, loss 0.0016910858289363704\n",
      "Epoch: 496, loss 0.0016871743125497266\n",
      "Epoch: 497, loss 0.0016832725180060887\n",
      "Epoch: 498, loss 0.001679380391920115\n",
      "Epoch: 499, loss 0.0016754978815386422\n",
      "Epoch: 500, loss 0.0016716249347321025\n",
      "Epoch: 501, loss 0.0016677614999860359\n",
      "Epoch: 502, loss 0.0016639075263926763\n",
      "Epoch: 503, loss 0.0016600629636427339\n",
      "Epoch: 504, loss 0.0016562277620172409\n",
      "Epoch: 505, loss 0.0016524018723795466\n",
      "Epoch: 506, loss 0.0016485852461673538\n",
      "Epoch: 507, loss 0.0016447778353849928\n",
      "Epoch: 508, loss 0.001640979592595664\n",
      "Epoch: 509, loss 0.0016371904709138166\n",
      "Epoch: 510, loss 0.0016334104239977922\n",
      "Epoch: 511, loss 0.001629639406042372\n",
      "Epoch: 512, loss 0.001625877371771424\n",
      "Epoch: 513, loss 0.0016221242764308656\n",
      "Epoch: 514, loss 0.0016183800757815126\n",
      "Epoch: 515, loss 0.001614644726092114\n",
      "Epoch: 516, loss 0.0016109181841324517\n",
      "Epoch: 517, loss 0.0016072004071666108\n",
      "Epoch: 518, loss 0.0016034913529462162\n",
      "Epoch: 519, loss 0.0015997909797038736\n",
      "Epoch: 520, loss 0.0015960992461466453\n",
      "Epoch: 521, loss 0.0015924161114496702\n",
      "Epoch: 522, loss 0.001588741535249793\n",
      "Epoch: 523, loss 0.0015850754776393077\n",
      "Epoch: 524, loss 0.0015814178991598398\n",
      "Epoch: 525, loss 0.0015777687607962316\n",
      "Epoch: 526, loss 0.0015741280239706629\n",
      "Epoch: 527, loss 0.0015704956505365622\n",
      "Epoch: 528, loss 0.0015668716027729256\n",
      "Epoch: 529, loss 0.0015632558433785327\n",
      "Epoch: 530, loss 0.0015596483354662933\n",
      "Epoch: 531, loss 0.0015560490425575823\n",
      "Epoch: 532, loss 0.0015524579285768351\n",
      "Epoch: 533, loss 0.0015488749578460414\n",
      "Epoch: 534, loss 0.0015453000950794457\n",
      "Epoch: 535, loss 0.0015417333053781668\n",
      "Epoch: 536, loss 0.0015381745542250773\n",
      "Epoch: 537, loss 0.0015346238074795768\n",
      "Epoch: 538, loss 0.0015310810313725848\n",
      "Epoch: 539, loss 0.0015275461925015035\n",
      "Epoch: 540, loss 0.0015240192578253031\n",
      "Epoch: 541, loss 0.0015205001946596015\n",
      "Epoch: 542, loss 0.0015441368816047863\n",
      "Epoch: 543, loss 0.0015580745867575195\n",
      "Epoch: 544, loss 0.0015354778147133792\n",
      "Epoch: 545, loss 0.0015207483779163017\n",
      "Epoch: 546, loss 0.0015107880065817387\n",
      "Epoch: 547, loss 0.0015307223457400527\n",
      "Epoch: 548, loss 0.00154225459313539\n",
      "Epoch: 549, loss 0.0015185481125527958\n",
      "Epoch: 550, loss 0.001503220818615008\n",
      "Epoch: 551, loss 0.0015200552139553048\n",
      "Epoch: 552, loss 0.0015296345768941743\n",
      "Epoch: 553, loss 0.0015050129700281461\n",
      "Epoch: 554, loss 0.001516122568240064\n",
      "Epoch: 555, loss 0.0015225506375093297\n",
      "Epoch: 556, loss 0.001496160247301116\n",
      "Epoch: 557, loss 0.0015062823943463662\n",
      "Epoch: 558, loss 0.001511975324567474\n",
      "Epoch: 559, loss 0.0014853525887989821\n",
      "Epoch: 560, loss 0.0014953986513773164\n",
      "Epoch: 561, loss 0.001500821939062729\n",
      "Epoch: 562, loss 0.0014742604482644735\n",
      "Epoch: 563, loss 0.001484402954865901\n",
      "Epoch: 564, loss 0.0014896468034628775\n",
      "Epoch: 565, loss 0.0014897991121699723\n",
      "Epoch: 566, loss 0.0014896761352851614\n",
      "Epoch: 567, loss 0.0014864356353895354\n",
      "Epoch: 568, loss 0.0014845004456628416\n",
      "Epoch: 569, loss 0.0014802071651708294\n",
      "Epoch: 570, loss 0.001477642166164339\n",
      "Epoch: 571, loss 0.0014730706265354682\n",
      "Epoch: 572, loss 0.0014702667172741767\n",
      "Epoch: 573, loss 0.0014656720543770147\n",
      "Epoch: 574, loss 0.0014627581474125813\n",
      "Epoch: 575, loss 0.0014582238792628529\n",
      "Epoch: 576, loss 0.0014552424103192198\n",
      "Epoch: 577, loss 0.0014507954541114973\n",
      "Epoch: 578, loss 0.0014477603653754246\n",
      "Epoch: 579, loss 0.0014434089926425833\n",
      "Epoch: 580, loss 0.0014403248575220876\n",
      "Epoch: 581, loss 0.0014360712033679413\n",
      "Epoch: 582, loss 0.0014329395273457836\n",
      "Epoch: 583, loss 0.0014287837093696523\n",
      "Epoch: 584, loss 0.0014256050036802683\n",
      "Epoch: 585, loss 0.0014215464771395695\n",
      "Epoch: 586, loss 0.0014183209403209915\n",
      "Epoch: 587, loss 0.0014143589436995173\n",
      "Epoch: 588, loss 0.0014110866850753927\n",
      "Epoch: 589, loss 0.0014072203865343892\n",
      "Epoch: 590, loss 0.0014039014991037307\n",
      "Epoch: 591, loss 0.001400130044646093\n",
      "Epoch: 592, loss 0.0013967646285151017\n",
      "Epoch: 593, loss 0.0013930871578768666\n",
      "Epoch: 594, loss 0.0013896753274706328\n",
      "Epoch: 595, loss 0.0013860909794220398\n",
      "Epoch: 596, loss 0.001382632865390239\n",
      "Epoch: 597, loss 0.0013791407795620833\n",
      "Epoch: 598, loss 0.0013756365289599908\n",
      "Epoch: 599, loss 0.0013722358465285888\n",
      "Epoch: 600, loss 0.0013941349607915904\n",
      "Epoch: 601, loss 0.001380211793279613\n",
      "Epoch: 602, loss 0.0013699321422550057\n",
      "Epoch: 603, loss 0.0013628618591597665\n",
      "Epoch: 604, loss 0.0013573882252379164\n",
      "Epoch: 605, loss 0.0013779786002410125\n",
      "Epoch: 606, loss 0.0013633973844464767\n",
      "Epoch: 607, loss 0.0013529368901875065\n",
      "Epoch: 608, loss 0.0013708003431042318\n",
      "Epoch: 609, loss 0.0013547730111958707\n",
      "Epoch: 610, loss 0.0013435011492668872\n",
      "Epoch: 611, loss 0.001360817896768171\n",
      "Epoch: 612, loss 0.0013445486449706293\n",
      "Epoch: 613, loss 0.0013332390998277517\n",
      "Epoch: 614, loss 0.0013504121435000127\n",
      "Epoch: 615, loss 0.001334122424627155\n",
      "Epoch: 616, loss 0.0013229167202835595\n",
      "Epoch: 617, loss 0.0013400200650668169\n",
      "Epoch: 618, loss 0.0013237497903614544\n",
      "Epoch: 619, loss 0.001312672807415406\n",
      "Epoch: 620, loss 0.0013297198212512788\n",
      "Epoch: 621, loss 0.0013134755294413481\n",
      "Epoch: 622, loss 0.001302530679118683\n",
      "Epoch: 623, loss 0.001319523589272158\n",
      "Epoch: 624, loss 0.0013033057533289444\n",
      "Epoch: 625, loss 0.0012924925022569209\n",
      "Epoch: 626, loss 0.0013094315034957729\n",
      "Epoch: 627, loss 0.0012932395502172262\n",
      "Epoch: 628, loss 0.0012825566417665817\n",
      "Epoch: 629, loss 0.0012994415689443076\n",
      "Epoch: 630, loss 0.0012832747973411736\n",
      "Epoch: 631, loss 0.0012727208431936642\n",
      "Epoch: 632, loss 0.0012895514774208694\n",
      "Epoch: 633, loss 0.0012734092238671812\n",
      "Epoch: 634, loss 0.0012629828128969715\n",
      "Epoch: 635, loss 0.0012797589370651468\n",
      "Epoch: 636, loss 0.0012879466940955808\n",
      "Epoch: 637, loss 0.0012669537845998334\n",
      "Epoch: 638, loss 0.0012535475041627194\n",
      "Epoch: 639, loss 0.0012686341234329547\n",
      "Epoch: 640, loss 0.0012758695137145488\n",
      "Epoch: 641, loss 0.0012544625457704228\n",
      "Epoch: 642, loss 0.001240902111403433\n",
      "Epoch: 643, loss 0.001255762552786468\n",
      "Epoch: 644, loss 0.0012628808147923732\n",
      "Epoch: 645, loss 0.001241515134975671\n",
      "Epoch: 646, loss 0.0012280889349903948\n",
      "Epoch: 647, loss 0.001242872789771392\n",
      "Epoch: 648, loss 0.0012499587399254484\n",
      "Epoch: 649, loss 0.0012286789941674426\n",
      "Epoch: 650, loss 0.001215414606586045\n",
      "Epoch: 651, loss 0.00123013670920684\n",
      "Epoch: 652, loss 0.0012371983227919346\n",
      "Epoch: 653, loss 0.0012160072897092812\n",
      "Epoch: 654, loss 0.0012029056286598202\n",
      "Epoch: 655, loss 0.0012175670790056747\n",
      "Epoch: 656, loss 0.0012246045312191518\n",
      "Epoch: 657, loss 0.0012271028755912587\n",
      "Epoch: 658, loss 0.0012035730396998214\n",
      "Epoch: 659, loss 0.001189099147966037\n",
      "Epoch: 660, loss 0.0012028827916975167\n",
      "Epoch: 661, loss 0.0012094354837430517\n",
      "Epoch: 662, loss 0.001211678435559251\n",
      "Epoch: 663, loss 0.0011881578364633528\n",
      "Epoch: 664, loss 0.0011737932463123874\n",
      "Epoch: 665, loss 0.0011874631757046452\n",
      "Epoch: 666, loss 0.0011939648614913295\n",
      "Epoch: 667, loss 0.0011961973360152642\n",
      "Epoch: 668, loss 0.0011728163605325657\n",
      "Epoch: 669, loss 0.001158642560712666\n",
      "Epoch: 670, loss 0.0011722414749814473\n",
      "Epoch: 671, loss 0.0011787157236243502\n",
      "Epoch: 672, loss 0.0011809503214589394\n",
      "Epoch: 673, loss 0.0011807967127113752\n",
      "Epoch: 674, loss 0.0011563111386128494\n",
      "Epoch: 675, loss 0.00114154437754566\n",
      "Epoch: 676, loss 0.001154643693588438\n",
      "Epoch: 677, loss 0.0011608507698785363\n",
      "Epoch: 678, loss 0.0011629559889425323\n",
      "Epoch: 679, loss 0.0011627532202213085\n",
      "Epoch: 680, loss 0.0011612582266403836\n",
      "Epoch: 681, loss 0.001136279019161018\n",
      "Epoch: 682, loss 0.0011213014671947565\n",
      "Epoch: 683, loss 0.0011340719295146833\n",
      "Epoch: 684, loss 0.0011401114640560335\n",
      "Epoch: 685, loss 0.0011421469922138683\n",
      "Epoch: 686, loss 0.0011419323002384784\n",
      "Epoch: 687, loss 0.001140459205884604\n",
      "Epoch: 688, loss 0.0011382855549633589\n",
      "Epoch: 689, loss 0.001113211354531796\n",
      "Epoch: 690, loss 0.0010982715212806957\n",
      "Epoch: 691, loss 0.0011108151002838596\n",
      "Epoch: 692, loss 0.0011167470054502927\n",
      "Epoch: 693, loss 0.0011187494241978954\n",
      "Epoch: 694, loss 0.0011185463882683743\n",
      "Epoch: 695, loss 0.0011171115518216973\n",
      "Epoch: 696, loss 0.0011149917850371575\n",
      "Epoch: 697, loss 0.0011124935901040695\n",
      "Epoch: 698, loss 0.0011097885247570297\n",
      "Epoch: 699, loss 0.0010847786578340127\n",
      "Epoch: 700, loss 0.00107679593307909\n",
      "Epoch: 701, loss 0.0010864483862737433\n",
      "Epoch: 702, loss 0.001090384721204054\n",
      "Epoch: 703, loss 0.0010912975945908622\n",
      "Epoch: 704, loss 0.00109052218873259\n",
      "Epoch: 705, loss 0.0010888068644312953\n",
      "Epoch: 706, loss 0.001086570811462725\n",
      "Epoch: 707, loss 0.0010840484442556266\n",
      "Epoch: 708, loss 0.0010813707046298737\n",
      "Epoch: 709, loss 0.0010786106747878571\n",
      "Epoch: 710, loss 0.0010758091118241108\n",
      "Epoch: 711, loss 0.001072988723785696\n",
      "Epoch: 712, loss 0.0010701621449559883\n",
      "Epoch: 713, loss 0.001045603978595408\n",
      "Epoch: 714, loss 0.0010377124686126089\n",
      "Epoch: 715, loss 0.0010472181605342239\n",
      "Epoch: 716, loss 0.0010511545894531101\n",
      "Epoch: 717, loss 0.0010521156420014068\n",
      "Epoch: 718, loss 0.0010514183963543722\n",
      "Epoch: 719, loss 0.0010497997512969098\n",
      "Epoch: 720, loss 0.0010476715340832828\n",
      "Epoch: 721, loss 0.0010452635113202544\n",
      "Epoch: 722, loss 0.001042703733344919\n",
      "Epoch: 723, loss 0.0010400635055494436\n",
      "Epoch: 724, loss 0.0010373825050181249\n",
      "Epoch: 725, loss 0.0010346827901116162\n",
      "Epoch: 726, loss 0.0010319766085538744\n",
      "Epoch: 727, loss 0.001029270746944416\n",
      "Epoch: 728, loss 0.0010265689527920471\n",
      "Epoch: 729, loss 0.001023873282859193\n",
      "Epoch: 730, loss 0.0010211848536136126\n",
      "Epoch: 731, loss 0.001018504258824941\n",
      "Epoch: 732, loss 0.0010158318018961311\n",
      "Epoch: 733, loss 0.001013167625097513\n",
      "Epoch: 734, loss 0.0009894591939644509\n",
      "Epoch: 735, loss 0.0009817878835329155\n",
      "Epoch: 736, loss 0.0009971995690671663\n",
      "Epoch: 737, loss 0.0009987578820040456\n",
      "Epoch: 738, loss 0.000998111188376813\n",
      "Epoch: 739, loss 0.0009965874129428424\n",
      "Epoch: 740, loss 0.0009945831163257924\n",
      "Epoch: 741, loss 0.000992315630067286\n",
      "Epoch: 742, loss 0.000989905649086516\n",
      "Epoch: 743, loss 0.0009874201652924788\n",
      "Epoch: 744, loss 0.0009848963369330603\n",
      "Epoch: 745, loss 0.0009823547579800621\n",
      "Epoch: 746, loss 0.000979806829207104\n",
      "Epoch: 747, loss 0.0009772588507200805\n",
      "Epoch: 748, loss 0.0009747142933881195\n",
      "Epoch: 749, loss 0.0009721750591914046\n",
      "Epoch: 750, loss 0.0009696421803941762\n",
      "Epoch: 751, loss 0.0009671162073311994\n",
      "Epoch: 752, loss 0.000964597423447982\n",
      "Epoch: 753, loss 0.0009620859645256013\n",
      "Epoch: 754, loss 0.0009595818847699274\n",
      "Epoch: 755, loss 0.0009570851934390439\n",
      "Epoch: 756, loss 0.000954595875137746\n",
      "Epoch: 757, loss 0.0009375617613775745\n",
      "Epoch: 758, loss 0.0009276392765734122\n",
      "Epoch: 759, loss 0.0009409300326764646\n",
      "Epoch: 760, loss 0.000927141613189982\n",
      "Epoch: 761, loss 0.0009189321710316349\n",
      "Epoch: 762, loss 0.0009329914863321195\n",
      "Epoch: 763, loss 0.0009196685593354618\n",
      "Epoch: 764, loss 0.0009118146944965238\n",
      "Epoch: 765, loss 0.0009259581454820929\n",
      "Epoch: 766, loss 0.000932839103551886\n",
      "Epoch: 767, loss 0.0009155626200961623\n",
      "Epoch: 768, loss 0.0009053360524223789\n",
      "Epoch: 769, loss 0.0009181607205168644\n",
      "Epoch: 770, loss 0.0009243118494291093\n",
      "Epoch: 771, loss 0.0009067409644682966\n",
      "Epoch: 772, loss 0.0008963931036084832\n",
      "Epoch: 773, loss 0.000909039231097778\n",
      "Epoch: 774, loss 0.0009150963218585321\n",
      "Epoch: 775, loss 0.0008975716788370144\n",
      "Epoch: 776, loss 0.0008873105444014362\n",
      "Epoch: 777, loss 0.0008998838495654064\n",
      "Epoch: 778, loss 0.0009059056487335973\n",
      "Epoch: 779, loss 0.0008884582269075668\n",
      "Epoch: 780, loss 0.0008783019334338832\n",
      "Epoch: 781, loss 0.0008908121757610724\n",
      "Epoch: 782, loss 0.0008968039850006184\n",
      "Epoch: 783, loss 0.0008794361514751441\n",
      "Epoch: 784, loss 0.0008693852612052791\n",
      "Epoch: 785, loss 0.0008818333437644861\n",
      "Epoch: 786, loss 0.0008877955432420002\n",
      "Epoch: 787, loss 0.0008705070721883957\n",
      "Epoch: 788, loss 0.0008605605341424433\n",
      "Epoch: 789, loss 0.0008729465525215965\n",
      "Epoch: 790, loss 0.0008788790813408853\n",
      "Epoch: 791, loss 0.000861669552417158\n",
      "Epoch: 792, loss 0.0008518261726002713\n",
      "Epoch: 793, loss 0.0008641501562399635\n",
      "Epoch: 794, loss 0.0008700529244389844\n",
      "Epoch: 795, loss 0.0008529219405539076\n",
      "Epoch: 796, loss 0.0008431805176328914\n",
      "Epoch: 797, loss 0.0008554424975688791\n",
      "Epoch: 798, loss 0.0008613154233545802\n",
      "Epoch: 799, loss 0.0008634726508839577\n",
      "Epoch: 800, loss 0.0008444784134144958\n",
      "Epoch: 801, loss 0.0008336487055565993\n",
      "Epoch: 802, loss 0.0008452288508471983\n",
      "Epoch: 803, loss 0.0008507308264725404\n",
      "Epoch: 804, loss 0.0008526940763526739\n",
      "Epoch: 805, loss 0.0008337328359798702\n",
      "Epoch: 806, loss 0.0008229674973939207\n",
      "Epoch: 807, loss 0.0008344463823276399\n",
      "Epoch: 808, loss 0.0008398990844928998\n",
      "Epoch: 809, loss 0.000841845606256675\n",
      "Epoch: 810, loss 0.0008230104700189422\n",
      "Epoch: 811, loss 0.0008123653719752062\n",
      "Epoch: 812, loss 0.0008237718187578053\n",
      "Epoch: 813, loss 0.0008291910052778843\n",
      "Epoch: 814, loss 0.0008311292325700927\n",
      "Epoch: 815, loss 0.0008124235062659224\n",
      "Epoch: 816, loss 0.0008018999555990558\n",
      "Epoch: 817, loss 0.0008132353885156809\n",
      "Epoch: 818, loss 0.0008186216577311194\n",
      "Epoch: 819, loss 0.0008205516817177074\n",
      "Epoch: 820, loss 0.0008019743221433425\n",
      "Epoch: 821, loss 0.0007915709290138699\n",
      "Epoch: 822, loss 0.0008028354335874487\n",
      "Epoch: 823, loss 0.0008081886577173\n",
      "Epoch: 824, loss 0.0008101101843824013\n",
      "Epoch: 825, loss 0.0008101500608987506\n",
      "Epoch: 826, loss 0.0007907425864816249\n",
      "Epoch: 827, loss 0.0007798716030908699\n",
      "Epoch: 828, loss 0.000790750263875306\n",
      "Epoch: 829, loss 0.000795898685052304\n",
      "Epoch: 830, loss 0.0007977199295085404\n",
      "Epoch: 831, loss 0.0007977186923779959\n",
      "Epoch: 832, loss 0.0007784602388908907\n",
      "Epoch: 833, loss 0.0007677123523792905\n",
      "Epoch: 834, loss 0.000778501953945062\n",
      "Epoch: 835, loss 0.0007836091536115962\n",
      "Epoch: 836, loss 0.0007854196657978613\n",
      "Epoch: 837, loss 0.0007854259741004972\n",
      "Epoch: 838, loss 0.0007663402968805274\n",
      "Epoch: 839, loss 0.0007557293640187579\n",
      "Epoch: 840, loss 0.0007664377894016356\n",
      "Epoch: 841, loss 0.0007715078502828329\n",
      "Epoch: 842, loss 0.0007733095423465551\n",
      "Epoch: 843, loss 0.00077332407854785\n",
      "Epoch: 844, loss 0.0007544096679294881\n",
      "Epoch: 845, loss 0.0007439342138561044\n",
      "Epoch: 846, loss 0.0007545617022243579\n",
      "Epoch: 847, loss 0.0007595945145449941\n",
      "Epoch: 848, loss 0.0007613870206045941\n",
      "Epoch: 849, loss 0.0007614092447842675\n",
      "Epoch: 850, loss 0.0007604692326778833\n",
      "Epoch: 851, loss 0.0007412547548658379\n",
      "Epoch: 852, loss 0.0007148771706939728\n",
      "Epoch: 853, loss 0.0007141661266795848\n",
      "Epoch: 854, loss 0.0007131417170982736\n",
      "Epoch: 855, loss 0.0007118049529966141\n",
      "Epoch: 856, loss 0.0007102987949780128\n",
      "Epoch: 857, loss 0.0007087020168798881\n",
      "Epoch: 858, loss 0.0007070572171283948\n",
      "Epoch: 859, loss 0.0007053874180377128\n",
      "Epoch: 860, loss 0.0007037050615249359\n",
      "Epoch: 861, loss 0.0007020168731012615\n",
      "Epoch: 862, loss 0.0007003264905771675\n",
      "Epoch: 863, loss 0.0006986358842705037\n",
      "Epoch: 864, loss 0.0006969461241097764\n",
      "Epoch: 865, loss 0.000695257793885743\n",
      "Epoch: 866, loss 0.0006935712149136341\n",
      "Epoch: 867, loss 0.0006918865667713288\n",
      "Epoch: 868, loss 0.0006902039524659114\n",
      "Epoch: 869, loss 0.0006885234336008073\n",
      "Epoch: 870, loss 0.0006868450493511387\n",
      "Epoch: 871, loss 0.0006851688267011059\n",
      "Epoch: 872, loss 0.0006834947859667071\n",
      "Epoch: 873, loss 0.0006818229437752187\n",
      "Epoch: 874, loss 0.0006801533146726735\n",
      "Epoch: 875, loss 0.0006784859119916023\n",
      "Epoch: 876, loss 0.0006768207483197652\n",
      "Epoch: 877, loss 0.0006751578357538952\n",
      "Epoch: 878, loss 0.0006734971860373875\n",
      "Epoch: 879, loss 0.0006718388106356222\n",
      "Epoch: 880, loss 0.00067018272077748\n",
      "Epoch: 881, loss 0.0006685289274787654\n",
      "Epoch: 882, loss 0.0006668774415558256\n",
      "Epoch: 883, loss 0.0006652282736338494\n",
      "Epoch: 884, loss 0.0006635814341523491\n",
      "Epoch: 885, loss 0.0006619369333690952\n",
      "Epoch: 886, loss 0.0006602947813631495\n",
      "Epoch: 887, loss 0.0006586549880375052\n",
      "Epoch: 888, loss 0.0006570175631214032\n",
      "Epoch: 889, loss 0.0006553825161725537\n",
      "Epoch: 890, loss 0.0006537498565791966\n",
      "Epoch: 891, loss 0.0006521195935621928\n",
      "Epoch: 892, loss 0.0006504917361769743\n",
      "Epoch: 893, loss 0.0006488662933155309\n",
      "Epoch: 894, loss 0.0006472432737083798\n",
      "Epoch: 895, loss 0.0006456226859263938\n",
      "Epoch: 896, loss 0.000644004538382765\n",
      "Epoch: 897, loss 0.0006423888393348622\n",
      "Epoch: 898, loss 0.0006407755968860114\n",
      "Epoch: 899, loss 0.0006391648189874022\n",
      "Epoch: 900, loss 0.0006375565134398396\n",
      "Epoch: 901, loss 0.0006359506878955643\n",
      "Epoch: 902, loss 0.0006343473498599506\n",
      "Epoch: 903, loss 0.0006327465066933217\n",
      "Epoch: 904, loss 0.0006311481656125898\n",
      "Epoch: 905, loss 0.0006295523336930369\n",
      "Epoch: 906, loss 0.0006279590178699476\n",
      "Epoch: 907, loss 0.0006263682249402975\n",
      "Epoch: 908, loss 0.0006247799615643764\n",
      "Epoch: 909, loss 0.0006231942342674168\n",
      "Epoch: 910, loss 0.0006216110494412108\n",
      "Epoch: 911, loss 0.0006200304133456832\n",
      "Epoch: 912, loss 0.0006184523321104942\n",
      "Epoch: 913, loss 0.0006168768117365406\n",
      "Epoch: 914, loss 0.0006153038580975117\n",
      "Epoch: 915, loss 0.0006137334769414233\n",
      "Epoch: 916, loss 0.000612165673892106\n",
      "Epoch: 917, loss 0.0006106004544506728\n",
      "Epoch: 918, loss 0.0006090378239969819\n",
      "Epoch: 919, loss 0.0006074777877911205\n",
      "Epoch: 920, loss 0.0006059203509747855\n",
      "Epoch: 921, loss 0.0006043655185727417\n",
      "Epoch: 922, loss 0.0006028132954941769\n",
      "Epoch: 923, loss 0.0006012636865341555\n",
      "Epoch: 924, loss 0.0005997166963748774\n",
      "Epoch: 925, loss 0.000598172329587134\n",
      "Epoch: 926, loss 0.0005966305906315683\n",
      "Epoch: 927, loss 0.000595091483860043\n",
      "Epoch: 928, loss 0.0005935550135169217\n",
      "Epoch: 929, loss 0.0005920211837403483\n",
      "Epoch: 930, loss 0.0005904899985635524\n",
      "Epoch: 931, loss 0.0005889614619160759\n",
      "Epoch: 932, loss 0.00058743557762504\n",
      "Epoch: 933, loss 0.0005859123494163682\n",
      "Epoch: 934, loss 0.0005843917809160046\n",
      "Epoch: 935, loss 0.0005828738756511205\n",
      "Epoch: 936, loss 0.0005813586370512858\n",
      "Epoch: 937, loss 0.0005798460684496704\n",
      "Epoch: 938, loss 0.0005783361730841744\n",
      "Epoch: 939, loss 0.0005768289540986088\n",
      "Epoch: 940, loss 0.0005753244145437956\n",
      "Epoch: 941, loss 0.0005738225573787317\n",
      "Epoch: 942, loss 0.0005723233854716501\n",
      "Epoch: 943, loss 0.0005708269016011662\n",
      "Epoch: 944, loss 0.0005693331084572909\n",
      "Epoch: 945, loss 0.0005678420086425881\n",
      "Epoch: 946, loss 0.0005663536046731435\n",
      "Epoch: 947, loss 0.000564867898979698\n",
      "Epoch: 948, loss 0.0005633848939086131\n",
      "Epoch: 949, loss 0.0005619045917229309\n",
      "Epoch: 950, loss 0.0005604269946033552\n",
      "Epoch: 951, loss 0.0005589521046492839\n",
      "Epoch: 952, loss 0.0005574799238797608\n",
      "Epoch: 953, loss 0.0005560104542344781\n",
      "Epoch: 954, loss 0.0005545436975747102\n",
      "Epoch: 955, loss 0.0005530796556843227\n",
      "Epoch: 956, loss 0.0005516183302706432\n",
      "Epoch: 957, loss 0.0005501597229654406\n",
      "Epoch: 958, loss 0.0005487038353258241\n",
      "Epoch: 959, loss 0.0005472506688351575\n",
      "Epoch: 960, loss 0.00054580022490397\n",
      "Epoch: 961, loss 0.0005443525048708319\n",
      "Epoch: 962, loss 0.0005429075100032162\n",
      "Epoch: 963, loss 0.0005414652414984186\n",
      "Epoch: 964, loss 0.0005400257004843448\n",
      "Epoch: 965, loss 0.000538588888020424\n",
      "Epoch: 966, loss 0.0005371548050984066\n",
      "Epoch: 967, loss 0.00053572345264321\n",
      "Epoch: 968, loss 0.0005342948315137235\n",
      "Epoch: 969, loss 0.0005328689425036125\n",
      "Epoch: 970, loss 0.0005314457863421722\n",
      "Epoch: 971, loss 0.0005048719795123203\n",
      "Epoch: 972, loss 0.0004883022223476477\n",
      "Epoch: 973, loss 0.000479238127394591\n",
      "Epoch: 974, loss 0.00047409065441166606\n",
      "Epoch: 975, loss 0.0004709900295374812\n",
      "Epoch: 976, loss 0.000468962702973039\n",
      "Epoch: 977, loss 0.0004674984599739476\n",
      "Epoch: 978, loss 0.00046632896921122144\n",
      "Epoch: 979, loss 0.0004653128703345006\n",
      "Epoch: 980, loss 0.00046437564470419617\n",
      "Epoch: 981, loss 0.00046347801191568027\n",
      "Epoch: 982, loss 0.0004625992811800413\n",
      "Epoch: 983, loss 0.00046172857096662306\n",
      "Epoch: 984, loss 0.00046086017594414587\n",
      "Epoch: 985, loss 0.00045999112184953484\n",
      "Epoch: 986, loss 0.0004591198750430813\n",
      "Epoch: 987, loss 0.0004582456615207207\n",
      "Epoch: 988, loss 0.0004573681075952262\n",
      "Epoch: 989, loss 0.00045648705033520274\n",
      "Epoch: 990, loss 0.0004556024375792891\n",
      "Epoch: 991, loss 0.0004547142752084173\n",
      "Epoch: 992, loss 0.0004538225993459709\n",
      "Epoch: 993, loss 0.0004529274617047697\n",
      "Epoch: 994, loss 0.00045202892186628805\n",
      "Epoch: 995, loss 0.0004426074258865096\n",
      "Epoch: 996, loss 0.00043318172976966016\n",
      "Epoch: 997, loss 0.00042795208311015444\n",
      "Epoch: 998, loss 0.0004249366064513081\n",
      "Epoch: 999, loss 0.0004230703166666299\n",
      "Epoch: 1000, loss 0.00042180050243208076\n"
     ]
    }
   ],
   "source": [
    "# we'll start building our neural network training app here\n",
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# we'll save these for future\n",
    "# se we can compare results to the final weights\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    " \n",
    "# learning rate and epochs\n",
    "LR = 0.005\n",
    "epochs = 1000\n",
    "\n",
    "# DataFrame data values as list\n",
    "data = list(df.values)\n",
    "\n",
    "# use min/max-scaling to make the values in the range 0 - 1\n",
    "data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "# let's initialize a list for loss points\n",
    "loss_points = []\n",
    "\n",
    "# START THE TRAINING PROCESS\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # let's also monitor epoch-wise losses\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # for example with first row\n",
    "        # [1, 0, 2] => assign input1 = 1, input2 = 0, true_value = 2\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # FORWARD PASS\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the the previous weights\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        # probably used later, we might want to have error metrics (MSE)\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current training data row loss to epoch losses\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solve w5 and update the new value\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        # solve w6 and update the new value\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        # solve bias3 and update the new value\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT FORWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES -> CHAIN RULE\n",
    "\n",
    "        # see the materials and the math experiment notebook for more details\n",
    "        # start with weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS!\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it the loss_points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the average loss of this epoch into the overall loss list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.7039042320114355\n",
      "w2: 1.5951916241126758\n",
      "w3: 0.8995182846606161\n",
      "w4: -0.146858661276722\n",
      "w5: 1.5358263108085535\n",
      "w6: 1.7366770439805095\n",
      "b1: 0.0012489593475154973\n",
      "b2: -0.10217002869707448\n",
      "b3: -0.027315593037430766\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # FORWARD PASS\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the the previous weights\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "x1     4\n",
       "x2     3\n",
       "y     22\n",
       "Name: 15, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15384615, 0.11538462, 0.84615385])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.39406121156953994"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # the value tends to be same as final bias3 \n",
    "# so if node1 and node2 outputs are small => more or less bias3\n",
    "result = predict(0.03846154, 0.23076923)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.245591500808038"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert scaled value back to actual Y\n",
    "df['y'].max() * result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
