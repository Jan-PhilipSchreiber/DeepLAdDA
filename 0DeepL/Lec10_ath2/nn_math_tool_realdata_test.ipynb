{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural network, experimentation tool, version 2</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# just copy/paste -the needed activation functions, \n",
    "# we're going to need these again\n",
    "\n",
    "# activation functions\n",
    "# ReLu is very simple, it filters out all negative numbers\n",
    "# this is a powerful activation function in reality\n",
    "def activation_ReLu(number):\n",
    "    if number > 0:\n",
    "        return number\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# we also need a derived version of ReLu later\n",
    "# otherwise the same than original, but instead of original value\n",
    "# return 1 instead\n",
    "def activation_ReLu_partial_derivative(number):\n",
    "    if number > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>47</td>\n",
       "      <td>45.320</td>\n",
       "      <td>8569.86180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>21</td>\n",
       "      <td>34.600</td>\n",
       "      <td>2020.17700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>19</td>\n",
       "      <td>26.030</td>\n",
       "      <td>16450.89470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2770</th>\n",
       "      <td>23</td>\n",
       "      <td>18.715</td>\n",
       "      <td>21595.38229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2771</th>\n",
       "      <td>54</td>\n",
       "      <td>31.600</td>\n",
       "      <td>9850.43200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2772 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi      charges\n",
       "0      19  27.900  16884.92400\n",
       "1      18  33.770   1725.55230\n",
       "2      28  33.000   4449.46200\n",
       "3      33  22.705  21984.47061\n",
       "4      32  28.880   3866.85520\n",
       "...   ...     ...          ...\n",
       "2767   47  45.320   8569.86180\n",
       "2768   21  34.600   2020.17700\n",
       "2769   19  26.030  16450.89470\n",
       "2770   23  18.715  21595.38229\n",
       "2771   54  31.600   9850.43200\n",
       "\n",
       "[2772 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"medical_insurance.csv\")\n",
    "df = df[[\"age\", \"bmi\", \"charges\"]]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The neural network training code</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss 0.002898427381287707\n",
      "Epoch: 2, loss 0.0028987276291954126\n",
      "Epoch: 3, loss 0.0028990274027002965\n",
      "Epoch: 4, loss 0.002899326673383366\n",
      "Epoch: 5, loss 0.002899625413951005\n",
      "Epoch: 6, loss 0.002899923598184759\n",
      "Epoch: 7, loss 0.002900221200893442\n",
      "Epoch: 8, loss 0.002900518197867183\n",
      "Epoch: 9, loss 0.0029008145658334307\n",
      "Epoch: 10, loss 0.002901110282414946\n",
      "Epoch: 11, loss 0.002901405326089596\n",
      "Epoch: 12, loss 0.0029016996761518994\n",
      "Epoch: 13, loss 0.0029019933126763655\n",
      "Epoch: 14, loss 0.002902286216482406\n",
      "Epoch: 15, loss 0.0029025783691009506\n",
      "Epoch: 16, loss 0.002902869752742521\n",
      "Epoch: 17, loss 0.0029031603502667966\n",
      "Epoch: 18, loss 0.002903450145153666\n",
      "Epoch: 19, loss 0.002903739121475606\n",
      "Epoch: 20, loss 0.0029040272638714408\n",
      "Epoch: 21, loss 0.002904314557521251\n",
      "Epoch: 22, loss 0.002904600988122704\n",
      "Epoch: 23, loss 0.0029048865418683407\n",
      "Epoch: 24, loss 0.0029051712054241484\n",
      "Epoch: 25, loss 0.00290545496590918\n",
      "Epoch: 26, loss 0.0029057378108761347\n",
      "Epoch: 27, loss 0.0029060197282930605\n",
      "Epoch: 28, loss 0.002906300706525894\n",
      "Epoch: 29, loss 0.002906580734321958\n",
      "Epoch: 30, loss 0.002906859800794372\n",
      "Epoch: 31, loss 0.0029071378954072004\n",
      "Epoch: 32, loss 0.0029074150079615225\n",
      "Epoch: 33, loss 0.0029076911285821215\n",
      "Epoch: 34, loss 0.0029079662477049907\n",
      "Epoch: 35, loss 0.0029082403560656014\n",
      "Epoch: 36, loss 0.002908513444687613\n",
      "Epoch: 37, loss 0.002908785504872459\n",
      "Epoch: 38, loss 0.002909056528189306\n",
      "Epoch: 39, loss 0.002909326506465813\n",
      "Epoch: 40, loss 0.002909595431779224\n",
      "Epoch: 41, loss 0.0029098632964481242\n",
      "Epoch: 42, loss 0.002910130093024604\n",
      "Epoch: 43, loss 0.00291039581428688\n",
      "Epoch: 44, loss 0.0029106604532324744\n",
      "Epoch: 45, loss 0.0029109240030716073\n",
      "Epoch: 46, loss 0.0029111864572212\n",
      "Epoch: 47, loss 0.0029114478092990724\n",
      "Epoch: 48, loss 0.0029117080531186456\n",
      "Epoch: 49, loss 0.002911967182683856\n",
      "Epoch: 50, loss 0.0029122251921843896\n",
      "Epoch: 51, loss 0.0029124820759913695\n",
      "Epoch: 52, loss 0.002912737828653073\n",
      "Epoch: 53, loss 0.002912992444891137\n",
      "Epoch: 54, loss 0.0029132459195968644\n",
      "Epoch: 55, loss 0.00291349824782783\n",
      "Epoch: 56, loss 0.002913749424804666\n",
      "Epoch: 57, loss 0.002913999445908099\n",
      "Epoch: 58, loss 0.0029142483066760742\n",
      "Epoch: 59, loss 0.002914496002801255\n",
      "Epoch: 60, loss 0.002914742530128375\n",
      "Epoch: 61, loss 0.0029149878846521263\n",
      "Epoch: 62, loss 0.0029152320625148095\n",
      "Epoch: 63, loss 0.0029154750600044366\n",
      "Epoch: 64, loss 0.002915716873552706\n",
      "Epoch: 65, loss 0.002915957499733285\n",
      "Epoch: 66, loss 0.0029161969352600177\n",
      "Epoch: 67, loss 0.002916435176985448\n",
      "Epoch: 68, loss 0.002916672221899173\n",
      "Epoch: 69, loss 0.0029169080671265215\n",
      "Epoch: 70, loss 0.0029171427099270795\n",
      "Epoch: 71, loss 0.0029173761476934484\n",
      "Epoch: 72, loss 0.0029176083779500705\n",
      "Epoch: 73, loss 0.0029178393983518994\n",
      "Epoch: 74, loss 0.002918069206683403\n",
      "Epoch: 75, loss 0.0029182978008574223\n",
      "Epoch: 76, loss 0.0029185251789140615\n",
      "Epoch: 77, loss 0.0029187513390197873\n",
      "Epoch: 78, loss 0.0029189762794663526\n",
      "Epoch: 79, loss 0.0029191999986698173\n",
      "Epoch: 80, loss 0.00291942249516969\n",
      "Epoch: 81, loss 0.002919643767627942\n",
      "Epoch: 82, loss 0.0029198638148280882\n",
      "Epoch: 83, loss 0.002920082635674363\n",
      "Epoch: 84, loss 0.002920300229190706\n",
      "Epoch: 85, loss 0.002920516594520034\n",
      "Epoch: 86, loss 0.0029207317309232236\n",
      "Epoch: 87, loss 0.0029209456377782966\n",
      "Epoch: 88, loss 0.0029211583145795773\n",
      "Epoch: 89, loss 0.0029213697609367195\n",
      "Epoch: 90, loss 0.0029215799765739277\n",
      "Epoch: 91, loss 0.0029217889613289785\n",
      "Epoch: 92, loss 0.002921996715152409\n",
      "Epoch: 93, loss 0.0029222032381065154\n",
      "Epoch: 94, loss 0.0029224085303645544\n",
      "Epoch: 95, loss 0.0029226125922097203\n",
      "Epoch: 96, loss 0.0029228154240342648\n",
      "Epoch: 97, loss 0.0029230170263385767\n",
      "Epoch: 98, loss 0.0029232173997301505\n",
      "Epoch: 99, loss 0.0029234165449226785\n",
      "Epoch: 100, loss 0.002923614462735081\n",
      "Epoch: 101, loss 0.0029238111540904254\n",
      "Epoch: 102, loss 0.00292400662001505\n",
      "Epoch: 103, loss 0.0029242008616374025\n",
      "Epoch: 104, loss 0.0029243938801870695\n",
      "Epoch: 105, loss 0.0029245856769937934\n",
      "Epoch: 106, loss 0.002924776253486288\n",
      "Epoch: 107, loss 0.002924965611191195\n",
      "Epoch: 108, loss 0.002925153751732078\n",
      "Epoch: 109, loss 0.002925340676828196\n",
      "Epoch: 110, loss 0.0029255263882934477\n",
      "Epoch: 111, loss 0.002925710888035265\n",
      "Epoch: 112, loss 0.002925894178053391\n",
      "Epoch: 113, loss 0.002926076260438799\n",
      "Epoch: 114, loss 0.002926257137372491\n",
      "Epoch: 115, loss 0.002926436811124302\n",
      "Epoch: 116, loss 0.002926615284051734\n",
      "Epoch: 117, loss 0.00292679255859877\n",
      "Epoch: 118, loss 0.00292696863729457\n",
      "Epoch: 119, loss 0.0029271435227524135\n",
      "Epoch: 120, loss 0.0029273172176682836\n",
      "Epoch: 121, loss 0.0029274897248197764\n",
      "Epoch: 122, loss 0.002927661047064754\n",
      "Epoch: 123, loss 0.002927831187340133\n",
      "Epoch: 124, loss 0.0029280001486606247\n",
      "Epoch: 125, loss 0.00292816793411742\n",
      "Epoch: 126, loss 0.002928334546876995\n",
      "Epoch: 127, loss 0.0029284999901797544\n",
      "Epoch: 128, loss 0.0029286642673387576\n",
      "Epoch: 129, loss 0.002928827381738493\n",
      "Epoch: 130, loss 0.002928989336833514\n",
      "Epoch: 131, loss 0.002929150136147142\n",
      "Epoch: 132, loss 0.0029293097832701984\n",
      "Epoch: 133, loss 0.0029294682818597115\n",
      "Epoch: 134, loss 0.002929625635637596\n",
      "Epoch: 135, loss 0.0029297818483893243\n",
      "Epoch: 136, loss 0.0029299369239626895\n",
      "Epoch: 137, loss 0.002930090866266409\n",
      "Epoch: 138, loss 0.0029302436792689366\n",
      "Epoch: 139, loss 0.0029303953669970175\n",
      "Epoch: 140, loss 0.002930545933534537\n",
      "Epoch: 141, loss 0.0029306953830211375\n",
      "Epoch: 142, loss 0.002930843719650918\n",
      "Epoch: 143, loss 0.002930990947671194\n",
      "Epoch: 144, loss 0.002931137071381174\n",
      "Epoch: 145, loss 0.0029312820951306866\n",
      "Epoch: 146, loss 0.0029314260233188755\n",
      "Epoch: 147, loss 0.0029315688603929885\n",
      "Epoch: 148, loss 0.002931710610847068\n",
      "Epoch: 149, loss 0.002931851279220661\n",
      "Epoch: 150, loss 0.002931990870097635\n",
      "Epoch: 151, loss 0.0029321293881048403\n",
      "Epoch: 152, loss 0.0029322668379110355\n",
      "Epoch: 153, loss 0.002932403224225386\n",
      "Epoch: 154, loss 0.0029325385517964975\n",
      "Epoch: 155, loss 0.0029326728254110545\n",
      "Epoch: 156, loss 0.002932806049892652\n",
      "Epoch: 157, loss 0.002932938230100605\n",
      "Epoch: 158, loss 0.002933069370928743\n",
      "Epoch: 159, loss 0.0029331994773042222\n",
      "Epoch: 160, loss 0.002933328554186372\n",
      "Epoch: 161, loss 0.0029334566065654898\n",
      "Epoch: 162, loss 0.0029335836394618208\n",
      "Epoch: 163, loss 0.0029337096579242494\n",
      "Epoch: 164, loss 0.0029338346670292523\n",
      "Epoch: 165, loss 0.0029339586718797757\n",
      "Epoch: 166, loss 0.002934081677604161\n",
      "Epoch: 167, loss 0.002934203689354966\n",
      "Epoch: 168, loss 0.002934324712307957\n",
      "Epoch: 169, loss 0.002934444751660996\n",
      "Epoch: 170, loss 0.0029345638126330263\n",
      "Epoch: 171, loss 0.002934681900462929\n",
      "Epoch: 172, loss 0.002934799020408625\n",
      "Epoch: 173, loss 0.0029349151777459384\n",
      "Epoch: 174, loss 0.0029350303777676196\n",
      "Epoch: 175, loss 0.0029351446257823875\n",
      "Epoch: 176, loss 0.002935257927113885\n",
      "Epoch: 177, loss 0.002935370287099726\n",
      "Epoch: 178, loss 0.0029354817110905924\n",
      "Epoch: 179, loss 0.002935592204449197\n",
      "Epoch: 180, loss 0.002935701772549398\n",
      "Epoch: 181, loss 0.0029358104207752853\n",
      "Epoch: 182, loss 0.002935918154520285\n",
      "Epoch: 183, loss 0.0029360249791862123\n",
      "Epoch: 184, loss 0.0029361309001824307\n",
      "Epoch: 185, loss 0.002936235922925008\n",
      "Epoch: 186, loss 0.0029363400528358264\n",
      "Epoch: 187, loss 0.0029364432953417276\n",
      "Epoch: 188, loss 0.002936545655873749\n",
      "Epoch: 189, loss 0.0029366471398662515\n",
      "Epoch: 190, loss 0.00293674775275615\n",
      "Epoch: 191, loss 0.002936847499982105\n",
      "Epoch: 192, loss 0.0029369463869837946\n",
      "Epoch: 193, loss 0.00293704441920113\n",
      "Epoch: 194, loss 0.002937141602073487\n",
      "Epoch: 195, loss 0.0029372379410389964\n",
      "Epoch: 196, loss 0.0029373334415338037\n",
      "Epoch: 197, loss 0.002937428108991412\n",
      "Epoch: 198, loss 0.0029375219488419714\n",
      "Epoch: 199, loss 0.00293761496651153\n",
      "Epoch: 200, loss 0.002937707167421466\n",
      "Epoch: 201, loss 0.0029377985569877738\n",
      "Epoch: 202, loss 0.002937889140620435\n",
      "Epoch: 203, loss 0.0029379789237228234\n",
      "Epoch: 204, loss 0.002938067911691025\n",
      "Epoch: 205, loss 0.002938156109913301\n",
      "Epoch: 206, loss 0.0029382435237694363\n",
      "Epoch: 207, loss 0.0029383301586302013\n",
      "Epoch: 208, loss 0.002938416019856817\n",
      "Epoch: 209, loss 0.002938501112800299\n",
      "Epoch: 210, loss 0.0029385854428010056\n",
      "Epoch: 211, loss 0.0029386690151881273\n",
      "Epoch: 212, loss 0.0029387518352790337\n",
      "Epoch: 213, loss 0.0029388339083789685\n",
      "Epoch: 214, loss 0.002938915239780352\n",
      "Epoch: 215, loss 0.0029389958347624313\n",
      "Epoch: 216, loss 0.002939075698590793\n",
      "Epoch: 217, loss 0.002939154836516869\n",
      "Epoch: 218, loss 0.002939233253777489\n",
      "Epoch: 219, loss 0.002939310955594512\n",
      "Epoch: 220, loss 0.002939387947174253\n",
      "Epoch: 221, loss 0.0029394642337072898\n",
      "Epoch: 222, loss 0.00293953982036781\n",
      "Epoch: 223, loss 0.0029396147123134414\n",
      "Epoch: 224, loss 0.0029396889146847327\n",
      "Epoch: 225, loss 0.0029397624326047873\n",
      "Epoch: 226, loss 0.002939835271179014\n",
      "Epoch: 227, loss 0.00293990743549462\n",
      "Epoch: 228, loss 0.0029399789306203627\n",
      "Epoch: 229, loss 0.002940049761606239\n",
      "Epoch: 230, loss 0.002940119933483096\n",
      "Epoch: 231, loss 0.0029401894512623475\n",
      "Epoch: 232, loss 0.002940258319935712\n",
      "Epoch: 233, loss 0.0029403265444748108\n",
      "Epoch: 234, loss 0.00294039412983102\n",
      "Epoch: 235, loss 0.0029404610809351073\n",
      "Epoch: 236, loss 0.0029405274026969843\n",
      "Epoch: 237, loss 0.002940593100005466\n",
      "Epoch: 238, loss 0.002940658177728014\n",
      "Epoch: 239, loss 0.002940722640710498\n",
      "Epoch: 240, loss 0.0029407864937769524\n",
      "Epoch: 241, loss 0.0029408497417293712\n",
      "Epoch: 242, loss 0.0029409123893474848\n",
      "Epoch: 243, loss 0.0029409744413885727\n",
      "Epoch: 244, loss 0.0029410359025872543\n",
      "Epoch: 245, loss 0.002941096777655256\n",
      "Epoch: 246, loss 0.0029411570712813014\n",
      "Epoch: 247, loss 0.0029412167881308774\n",
      "Epoch: 248, loss 0.0029412759328461377\n",
      "Epoch: 249, loss 0.0029413345100456454\n",
      "Epoch: 250, loss 0.0029413925243243163\n",
      "Epoch: 251, loss 0.0029414499802531777\n",
      "Epoch: 252, loss 0.0029415068823793428\n",
      "Epoch: 253, loss 0.00294156323522574\n",
      "Epoch: 254, loss 0.002941619043291166\n",
      "Epoch: 255, loss 0.002941674311049985\n",
      "Epoch: 256, loss 0.0029417290429521125\n",
      "Epoch: 257, loss 0.002941783243422912\n",
      "Epoch: 258, loss 0.002941836916863088\n",
      "Epoch: 259, loss 0.00294189006764854\n",
      "Epoch: 260, loss 0.002941942700130348\n",
      "Epoch: 261, loss 0.0029419948186346833\n",
      "Epoch: 262, loss 0.002942046427462668\n",
      "Epoch: 263, loss 0.0029420975308903654\n",
      "Epoch: 264, loss 0.0029421481331687017\n",
      "Epoch: 265, loss 0.0029421982385234007\n",
      "Epoch: 266, loss 0.0029422478511549515\n",
      "Epoch: 267, loss 0.002942296975238494\n",
      "Epoch: 268, loss 0.0029423456149238647\n",
      "Epoch: 269, loss 0.0029423937743355347\n",
      "Epoch: 270, loss 0.0029424414575724635\n",
      "Epoch: 271, loss 0.0029424886687083006\n",
      "Epoch: 272, loss 0.0029425354117911355\n",
      "Epoch: 273, loss 0.002942581690843556\n",
      "Epoch: 274, loss 0.0029426275098627225\n",
      "Epoch: 275, loss 0.0029426728728202617\n",
      "Epoch: 276, loss 0.0029427177836622246\n",
      "Epoch: 277, loss 0.002942762246309194\n",
      "Epoch: 278, loss 0.0029428062646562276\n",
      "Epoch: 279, loss 0.002942849842572873\n",
      "Epoch: 280, loss 0.002942892983903144\n",
      "Epoch: 281, loss 0.0029429356924656077\n",
      "Epoch: 282, loss 0.002942977972053338\n",
      "Epoch: 283, loss 0.0029430198264339797\n",
      "Epoch: 284, loss 0.0029430612593497442\n",
      "Epoch: 285, loss 0.0029431022745174235\n",
      "Epoch: 286, loss 0.0029431428756284657\n",
      "Epoch: 287, loss 0.002943183066349046\n",
      "Epoch: 288, loss 0.002943222850319952\n",
      "Epoch: 289, loss 0.0029432622311567932\n",
      "Epoch: 290, loss 0.002943301212449954\n",
      "Epoch: 291, loss 0.0029433397977646734\n",
      "Epoch: 292, loss 0.0029433779906410658\n",
      "Epoch: 293, loss 0.0029434157945942186\n",
      "Epoch: 294, loss 0.002943453213114209\n",
      "Epoch: 295, loss 0.0029434902496661775\n",
      "Epoch: 296, loss 0.00294352690769043\n",
      "Epoch: 297, loss 0.0029435631906023735\n",
      "Epoch: 298, loss 0.0029435991017927963\n",
      "Epoch: 299, loss 0.002943634644627708\n",
      "Epoch: 300, loss 0.0029436698224485198\n"
     ]
    }
   ],
   "source": [
    "# we'll start building our neural network training app here\n",
    "# initialize weights and biases\n",
    "# in Keras etc. these are usually randomized in the beginning\n",
    "w1 = 1\n",
    "w2 = 0.5\n",
    "w3 = 1\n",
    "w4 = -0.5\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "bias1 = 0.5\n",
    "bias2 = 0\n",
    "bias3 = 0.5\n",
    "\n",
    "# we'll save these for future\n",
    "# se we can compare results to the final weights\n",
    "original_w1 = w1\n",
    "original_w2 = w2\n",
    "original_w3 = w3\n",
    "original_w4 = w4\n",
    "original_w5 = w5\n",
    "original_w6 = w6\n",
    "original_b1 = bias1\n",
    "original_b2 = bias2\n",
    "original_b3 = bias3\n",
    " \n",
    "# learning rate and epochs\n",
    "LR = 0.005\n",
    "epochs = 300\n",
    "\n",
    "# DataFrame data values as list\n",
    "data = list(df.values)\n",
    "\n",
    "# use min/max-scaling to make the values in the range 0 - 1\n",
    "data = (data - np.min(data)) /(np.max(data) - np.min(data))\n",
    "\n",
    "# let's initialize a list for loss points\n",
    "loss_points = []\n",
    "\n",
    "# START THE TRAINING PROCESS\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # let's also monitor epoch-wise losses\n",
    "    epoch_losses = []\n",
    "\n",
    "    for row in data:\n",
    "        # for example with first row\n",
    "        # [1, 0, 2] => assign input1 = 1, input2 = 0, true_value = 2\n",
    "        input1 = row[0]\n",
    "        input2 = row[1]\n",
    "        true_value = row[2]\n",
    "\n",
    "        # FORWARD PASS\n",
    "\n",
    "        # NODE 1 OUTPUT\n",
    "        node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "        node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "        # NODE 2 OUTPUT\n",
    "        node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "        node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "        # NODE 3 OUTPUT\n",
    "        # we can just use Node 1 and 2 outputs, since they\n",
    "        # already contain the the previous weights\n",
    "        node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "        node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "        # probably used later, we might want to have error metrics (MSE)\n",
    "        predicted_value = node_3_output\n",
    "        loss = (predicted_value - true_value) ** 2\n",
    "\n",
    "        # add current training data row loss to epoch losses\n",
    "        epoch_losses.append(loss)\n",
    "\n",
    "        # BACKPROPAGATION - LAST LAYER FIRST\n",
    "        # solve w5 and update the new value\n",
    "        deriv_L_w5 = 2 * node_1_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w5 = w5 - LR * deriv_L_w5\n",
    "\n",
    "        # solve w6 and update the new value\n",
    "        deriv_L_w6 = 2 * node_2_output * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_w6 = w6 - LR * deriv_L_w6\n",
    "\n",
    "        # solve bias3 and update the new value\n",
    "        deriv_L_b3 = 2 * 1 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        new_b3 = bias3 - LR * deriv_L_b3\n",
    "\n",
    "        # BACKPROPAGATION - THE FIRST LAYER\n",
    "        # FROM THIS POINT FORWARD WE HAVE TO USE THE MORE COMPLEX VERSION\n",
    "        # OF UPDATING THE VALUES -> CHAIN RULE\n",
    "\n",
    "        # see the materials and the math experiment notebook for more details\n",
    "        # start with weight 1\n",
    "        deriv_L_w1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input1\n",
    "        deriv_L_w1 = deriv_L_w1_left * deriv_L_w1_right\n",
    "        new_w1 = w1 - LR * deriv_L_w1\n",
    "\n",
    "        # weight 2\n",
    "        deriv_L_w2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input1\n",
    "        deriv_L_w2 = deriv_L_w2_left * deriv_L_w2_right\n",
    "        new_w2 = w2 - LR * deriv_L_w2\n",
    "\n",
    "        # weight 3\n",
    "        deriv_L_w3_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w3_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * input2\n",
    "        deriv_L_w3 = deriv_L_w3_left * deriv_L_w3_right\n",
    "        new_w3 = w3 - LR * deriv_L_w3\n",
    "\n",
    "        # weight 4\n",
    "        deriv_L_w4_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_w4_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * input2\n",
    "        deriv_L_w4 = deriv_L_w4_left * deriv_L_w4_right\n",
    "        new_w4 = w4 - LR * deriv_L_w4\n",
    "\n",
    "        # bias 1\n",
    "        deriv_L_b1_left = 2 * w5 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b1_right = activation_ReLu_partial_derivative(input1 * w1 + input2 * w3 + bias1) * 1\n",
    "        deriv_L_b1 = deriv_L_b1_left * deriv_L_b1_right\n",
    "        new_b1 = bias1 - LR * deriv_L_b1\n",
    "\n",
    "        # bias 2\n",
    "        deriv_L_b2_left = 2 * w6 * (node_1_output * w5 + node_2_output * w6 + bias3 - true_value)\n",
    "        deriv_L_b2_right = activation_ReLu_partial_derivative(input1 * w2 + input2 * w4 + bias2) * 1\n",
    "        deriv_L_b2 = deriv_L_b2_left * deriv_L_b2_right\n",
    "        new_b2 = bias2 - LR * deriv_L_b2\n",
    "\n",
    "        # ALL DONE! FINALLY UPDATE THE EXISTING WEIGHTS!\n",
    "        w1 = new_w1\n",
    "        w2 = new_w2\n",
    "        w3 = new_w3\n",
    "        w4 = new_w4\n",
    "        w5 = new_w5\n",
    "        w6 = new_w6\n",
    "        bias1 = new_b1\n",
    "        bias2 = new_b2\n",
    "        bias3 = new_b3\n",
    "\n",
    "    # calculate average epoch-wise loss and add it the loss_points\n",
    "    average_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "\n",
    "    # place the average loss of this epoch into the overall loss list\n",
    "    loss_points.append(average_loss)\n",
    "    print(f\"Epoch: {epoch +1}, loss {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL WEIGHTS AND BIASES\n",
      "w1: 1\n",
      "w2: 0.5\n",
      "w3: 1\n",
      "w4: -0.5\n",
      "w5: 1\n",
      "w6: 1\n",
      "b1: 0.5\n",
      "b2: 0\n",
      "b3: 0.5\n",
      "\n",
      "\n",
      "######################################\n",
      "NEW WEIGHTS AND BIASES\n",
      "w1: 1.0514572358400067\n",
      "w2: 0.49999840106287474\n",
      "w3: 1.014988333438993\n",
      "w4: -0.5000006329126121\n",
      "w5: 0.2658051321730554\n",
      "w6: 0.9999995169877434\n",
      "b1: 0.04838695218534944\n",
      "b2: -0.00598235832164403\n",
      "b3: 0.19483435509371008\n"
     ]
    }
   ],
   "source": [
    "print(\"ORIGINAL WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {original_w1}\")\n",
    "print(f\"w2: {original_w2}\")\n",
    "print(f\"w3: {original_w3}\")\n",
    "print(f\"w4: {original_w4}\")\n",
    "print(f\"w5: {original_w5}\")\n",
    "print(f\"w6: {original_w6}\")\n",
    "print(f\"b1: {original_b1}\")\n",
    "print(f\"b2: {original_b2}\")\n",
    "print(f\"b3: {original_b3}\")\n",
    "\n",
    "print(\"\\n\\n######################################\")\n",
    "\n",
    "print(\"NEW WEIGHTS AND BIASES\")\n",
    "print(f\"w1: {w1}\")\n",
    "print(f\"w2: {w2}\")\n",
    "print(f\"w3: {w3}\")\n",
    "print(f\"w4: {w4}\")\n",
    "print(f\"w5: {w5}\")\n",
    "print(f\"w6: {w6}\")\n",
    "print(f\"b1: {bias1}\")\n",
    "print(f\"b2: {bias2}\")\n",
    "print(f\"b3: {bias3}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_points)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function, just doing the forward pass\n",
    "# again (but only that)\n",
    "def predict(x1, x2):\n",
    "    input1 = x1\n",
    "    input2 = x2\n",
    "\n",
    "    # FORWARD PASS\n",
    "\n",
    "    # NODE 1 OUTPUT\n",
    "    node_1_output = input1 * w1 + input2 * w3 + bias1\n",
    "    node_1_output = activation_ReLu(node_1_output)\n",
    "\n",
    "    # NODE 2 OUTPUT\n",
    "    node_2_output = input1 * w2 + input2 * w4 + bias2\n",
    "    node_2_output = activation_ReLu(node_2_output)\n",
    "\n",
    "    # NODE 3 OUTPUT\n",
    "    # we can just use Node 1 and 2 outputs, since they\n",
    "    # already contain the the previous weights\n",
    "    node_3_output = node_1_output * w5 + node_2_output * w6 + bias3\n",
    "    node_3_output = activation_ReLu(node_3_output)\n",
    "\n",
    "    return node_3_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age           64.0000\n",
       "bmi           39.3300\n",
       "charges    14901.5167\n",
       "Name: 199, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the actual values\n",
    "df.iloc[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00075352, 0.00036656, 0.23348256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the normalized values\n",
    "data[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll try using some values from the original training data\n",
    "result = predict(0.8, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41266.6937350033"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert scaled value back to actual USD\n",
    "df['charges'].max() * result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "      <td>2772.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.109668</td>\n",
       "      <td>30.701349</td>\n",
       "      <td>13261.369959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.081459</td>\n",
       "      <td>6.129449</td>\n",
       "      <td>12151.768945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>15.960000</td>\n",
       "      <td>1121.873900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>26.220000</td>\n",
       "      <td>4687.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>39.000000</td>\n",
       "      <td>30.447500</td>\n",
       "      <td>9333.014350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>34.770000</td>\n",
       "      <td>16577.779500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>64.000000</td>\n",
       "      <td>53.130000</td>\n",
       "      <td>63770.428010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age          bmi       charges\n",
       "count  2772.000000  2772.000000   2772.000000\n",
       "mean     39.109668    30.701349  13261.369959\n",
       "std      14.081459     6.129449  12151.768945\n",
       "min      18.000000    15.960000   1121.873900\n",
       "25%      26.000000    26.220000   4687.797000\n",
       "50%      39.000000    30.447500   9333.014350\n",
       "75%      51.000000    34.770000  16577.779500\n",
       "max      64.000000    53.130000  63770.428010"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
